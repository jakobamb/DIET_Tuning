WandB initialized: DIET_dinov2_small_cifar10_20250419_155824

==================================================
INITIAL ZERO-SHOT EVALUATION (BEFORE TRAINING)
==================================================
Extracting features for zero-shot evaluation...
Extracting features:   0%|          | 0/500 [00:00<?, ?it/s]
Feature hash: -6430982427924699850 (should change between evaluations)
Features extracted: (10000, 384), time: 20.27s
Running k-NN evaluation...
k-NN accuracy: 0.9414, time: 0.44s
Running k-means clustering evaluation...
k-means ARI: 0.6247, NMI: 0.7445, time: 1.18s
Running linear probe evaluation...
Linear probe accuracy: 0.9240, time: 0.17s
Total zero-shot evaluation time: 22.06s
Initial evaluation completed in 22.06s

==========================
Starting epoch 1/30 at 15:58:47
==========================

Initializing training loop...


Batch 0: grad_norm=99.9566 across 22310154 trainable params
Batch 10: grad_norm=8.4240 across 22310154 trainable params
Batch 20: grad_norm=6.0607 across 22310154 trainable params
Batch 30: grad_norm=5.1222 across 22310154 trainable params
Batch 40: grad_norm=4.0812 across 22310154 trainable params

Epoch 1 completed in 7.43s

Learning rate updated to: 0.000499
Epoch 1 Metrics - DIET Loss: 2.3088e+00, Probe Loss: 3.2195e+00, Accuracy: 0.1000

Starting evaluation on test set:
Test Batch 0: Accuracy=0.2000
Test Batch 10: Accuracy=0.1500
Test Batch 20: Accuracy=0.2000
Test Batch 30: Accuracy=0.2000
Test Batch 40: Accuracy=0.1500
Test Batch 50: Accuracy=0.2000
Test Batch 60: Accuracy=0.2500
Test Batch 70: Accuracy=0.1000
Test Batch 80: Accuracy=0.1000
Test Batch 90: Accuracy=0.1000
Test Batch 100: Accuracy=0.1000
Test Batch 110: Accuracy=0.0500
Test Batch 120: Accuracy=0.0500
Test Batch 130: Accuracy=0.1000
Test Batch 140: Accuracy=0.0000
Test Batch 150: Accuracy=0.0500
Test Batch 160: Accuracy=0.1000
Test Batch 170: Accuracy=0.0500
Test Batch 180: Accuracy=0.0000
Test Batch 190: Accuracy=0.2000
Test Batch 200: Accuracy=0.2000
Test Batch 210: Accuracy=0.0000
Test Batch 220: Accuracy=0.1000
Test Batch 230: Accuracy=0.0500
Test Batch 240: Accuracy=0.1500
Test Batch 250: Accuracy=0.0500
Test Batch 260: Accuracy=0.1000
Test Batch 270: Accuracy=0.0500
Test Batch 280: Accuracy=0.2000
Test Batch 290: Accuracy=0.0500
Test Batch 300: Accuracy=0.0500
Test Batch 310: Accuracy=0.1500
Test Batch 320: Accuracy=0.2000
Test Batch 330: Accuracy=0.2000
Test Batch 340: Accuracy=0.0500
Test Batch 350: Accuracy=0.0000
Test Batch 360: Accuracy=0.0500
Test Batch 370: Accuracy=0.2000
Test Batch 380: Accuracy=0.1000
Test Batch 390: Accuracy=0.0500
Test Batch 400: Accuracy=0.2000
Test Batch 410: Accuracy=0.0500
Test Batch 420: Accuracy=0.2000
Test Batch 430: Accuracy=0.2000
Test Batch 440: Accuracy=0.0500
Test Batch 450: Accuracy=0.2500
Test Batch 460: Accuracy=0.1500
Test Batch 470: Accuracy=0.1500
Test Batch 480: Accuracy=0.2500
Test Batch 490: Accuracy=0.1500

Overall Test Accuracy: 0.1000
Epoch 1/30 summary:
  Train - DIET loss: 2.3088e+00, Probe loss: 3.2195e+00, Acc: 0.1000
  Test  - Acc: 0.1000

==========================
Starting epoch 2/30 at 15:59:17
==========================

Initializing training loop...


Batch 0: grad_norm=3.1109 across 22310154 trainable params
Batch 10: grad_norm=2.8529 across 22310154 trainable params
Batch 20: grad_norm=3.8656 across 22310154 trainable params
Batch 30: grad_norm=3.3842 across 22310154 trainable params
Batch 40: grad_norm=3.9420 across 22310154 trainable params

Epoch 2 completed in 7.40s

Learning rate updated to: 0.000495
Epoch 2 Metrics - DIET Loss: 2.3017e+00, Probe Loss: 2.3395e+00, Accuracy: 0.1240

Starting evaluation on test set:
Test Batch 0: Accuracy=0.1500
Test Batch 10: Accuracy=0.2000
Test Batch 20: Accuracy=0.2500
Test Batch 30: Accuracy=0.0500
Test Batch 40: Accuracy=0.1000
Test Batch 50: Accuracy=0.1000
Test Batch 60: Accuracy=0.2500
Test Batch 70: Accuracy=0.1500
Test Batch 80: Accuracy=0.1500
Test Batch 90: Accuracy=0.0500
Test Batch 100: Accuracy=0.1000
Test Batch 110: Accuracy=0.0500
Test Batch 120: Accuracy=0.0500
Test Batch 130: Accuracy=0.2000
Test Batch 140: Accuracy=0.0500
Test Batch 150: Accuracy=0.1000
Test Batch 160: Accuracy=0.1000
Test Batch 170: Accuracy=0.1000
Test Batch 180: Accuracy=0.0000
Test Batch 190: Accuracy=0.1000
Test Batch 200: Accuracy=0.0500
Test Batch 210: Accuracy=0.1000
Test Batch 220: Accuracy=0.1000
Test Batch 230: Accuracy=0.0500
Test Batch 240: Accuracy=0.1000
Test Batch 250: Accuracy=0.2000
Test Batch 260: Accuracy=0.1500
Test Batch 270: Accuracy=0.1000
Test Batch 280: Accuracy=0.0500
Test Batch 290: Accuracy=0.1500
Test Batch 300: Accuracy=0.1500
Test Batch 310: Accuracy=0.2000
Test Batch 320: Accuracy=0.2500
Test Batch 330: Accuracy=0.2500
Test Batch 340: Accuracy=0.1500
Test Batch 350: Accuracy=0.2500
Test Batch 360: Accuracy=0.0500
Test Batch 370: Accuracy=0.1000
Test Batch 380: Accuracy=0.2000
Test Batch 390: Accuracy=0.2500
Test Batch 400: Accuracy=0.1500
Test Batch 410: Accuracy=0.1500
Test Batch 420: Accuracy=0.1000
Test Batch 430: Accuracy=0.1000
Test Batch 440: Accuracy=0.2000
Test Batch 450: Accuracy=0.1500
Test Batch 460: Accuracy=0.1000
Test Batch 470: Accuracy=0.1000
Test Batch 480: Accuracy=0.1000
Test Batch 490: Accuracy=0.1000

Overall Test Accuracy: 0.1325
Epoch 2/30 summary:
  Train - DIET loss: 2.3017e+00, Probe loss: 2.3395e+00, Acc: 0.1240
  Test  - Acc: 0.1325

==========================
Starting epoch 3/30 at 15:59:47
==========================

Initializing training loop...


Batch 0: grad_norm=5.0060 across 22310154 trainable params
Batch 10: grad_norm=2.4985 across 22310154 trainable params
Batch 20: grad_norm=4.7770 across 22310154 trainable params
Batch 30: grad_norm=3.5072 across 22310154 trainable params
Batch 40: grad_norm=4.9393 across 22310154 trainable params

Epoch 3 completed in 7.50s

Learning rate updated to: 0.000488
Epoch 3 Metrics - DIET Loss: 2.3146e+00, Probe Loss: 2.2578e+00, Accuracy: 0.1710

Starting evaluation on test set:
Test Batch 0: Accuracy=0.1500
Test Batch 10: Accuracy=0.2000
Test Batch 20: Accuracy=0.1000
Test Batch 30: Accuracy=0.1500
Test Batch 40: Accuracy=0.1500
Test Batch 50: Accuracy=0.2000
Test Batch 60: Accuracy=0.3000
Test Batch 70: Accuracy=0.0500
Test Batch 80: Accuracy=0.0000
Test Batch 90: Accuracy=0.3000
Test Batch 100: Accuracy=0.3000
Test Batch 110: Accuracy=0.2000
Test Batch 120: Accuracy=0.0500
Test Batch 130: Accuracy=0.2000
Test Batch 140: Accuracy=0.1500
Test Batch 150: Accuracy=0.2500
Test Batch 160: Accuracy=0.1500
Test Batch 170: Accuracy=0.2000
Test Batch 180: Accuracy=0.1000
Test Batch 190: Accuracy=0.1500
Test Batch 200: Accuracy=0.1500
Test Batch 210: Accuracy=0.0000
Test Batch 220: Accuracy=0.1000
Test Batch 230: Accuracy=0.1000
Test Batch 240: Accuracy=0.2000
Test Batch 250: Accuracy=0.1000
Test Batch 260: Accuracy=0.1500
Test Batch 270: Accuracy=0.0000
Test Batch 280: Accuracy=0.2500
Test Batch 290: Accuracy=0.1500
Test Batch 300: Accuracy=0.1500
Test Batch 310: Accuracy=0.2000
Test Batch 320: Accuracy=0.0000
Test Batch 330: Accuracy=0.1500
Test Batch 340: Accuracy=0.1500
Test Batch 350: Accuracy=0.0000
Test Batch 360: Accuracy=0.1000
Test Batch 370: Accuracy=0.1500
Test Batch 380: Accuracy=0.1000
Test Batch 390: Accuracy=0.1000
Test Batch 400: Accuracy=0.0500
Test Batch 410: Accuracy=0.1500
Test Batch 420: Accuracy=0.2000
Test Batch 430: Accuracy=0.1000
Test Batch 440: Accuracy=0.0000
Test Batch 450: Accuracy=0.1500
Test Batch 460: Accuracy=0.2000
Test Batch 470: Accuracy=0.2000
Test Batch 480: Accuracy=0.2000
Test Batch 490: Accuracy=0.1500

Overall Test Accuracy: 0.1429
Epoch 3/30 summary:
  Train - DIET loss: 2.3146e+00, Probe loss: 2.2578e+00, Acc: 0.1710
  Test  - Acc: 0.1429

==========================
Starting epoch 4/30 at 16:00:19
==========================

Initializing training loop...


Batch 0: grad_norm=5.6282 across 22310154 trainable params
Batch 10: grad_norm=2.1977 across 22310154 trainable params
Batch 20: grad_norm=3.2763 across 22310154 trainable params
Batch 30: grad_norm=4.7378 across 22310154 trainable params
Batch 40: grad_norm=3.8894 across 22310154 trainable params

Epoch 4 completed in 7.57s

Learning rate updated to: 0.000479
Epoch 4 Metrics - DIET Loss: 2.3091e+00, Probe Loss: 2.2106e+00, Accuracy: 0.1910

Starting evaluation on test set:
Test Batch 0: Accuracy=0.2000
Test Batch 10: Accuracy=0.2500
Test Batch 20: Accuracy=0.1500
Test Batch 30: Accuracy=0.2500
Test Batch 40: Accuracy=0.2000
Test Batch 50: Accuracy=0.1500
Test Batch 60: Accuracy=0.3500
Test Batch 70: Accuracy=0.1000
Test Batch 80: Accuracy=0.0500
Test Batch 90: Accuracy=0.2500
Test Batch 100: Accuracy=0.1500
Test Batch 110: Accuracy=0.2000
Test Batch 120: Accuracy=0.1000
Test Batch 130: Accuracy=0.3500
Test Batch 140: Accuracy=0.1000
Test Batch 150: Accuracy=0.0500
Test Batch 160: Accuracy=0.1000
Test Batch 170: Accuracy=0.3000
Test Batch 180: Accuracy=0.1000
Test Batch 190: Accuracy=0.2000
Test Batch 200: Accuracy=0.3000
Test Batch 210: Accuracy=0.2000
Test Batch 220: Accuracy=0.1500
Test Batch 230: Accuracy=0.1500
Test Batch 240: Accuracy=0.3000
Test Batch 250: Accuracy=0.1500
Test Batch 260: Accuracy=0.0500
Test Batch 270: Accuracy=0.1500
Test Batch 280: Accuracy=0.2000
Test Batch 290: Accuracy=0.1500
Test Batch 300: Accuracy=0.1500
Test Batch 310: Accuracy=0.1500
Test Batch 320: Accuracy=0.2000
Test Batch 330: Accuracy=0.3000
Test Batch 340: Accuracy=0.2000
Test Batch 350: Accuracy=0.1000
Test Batch 360: Accuracy=0.1500
Test Batch 370: Accuracy=0.2000
Test Batch 380: Accuracy=0.2500
Test Batch 390: Accuracy=0.2500
Test Batch 400: Accuracy=0.2000
Test Batch 410: Accuracy=0.3000
Test Batch 420: Accuracy=0.3500
Test Batch 430: Accuracy=0.3000
Test Batch 440: Accuracy=0.0000
Test Batch 450: Accuracy=0.4500
Test Batch 460: Accuracy=0.2500
Test Batch 470: Accuracy=0.2000
Test Batch 480: Accuracy=0.3500
Test Batch 490: Accuracy=0.1000

Overall Test Accuracy: 0.1938
Epoch 4/30 summary:
  Train - DIET loss: 2.3091e+00, Probe loss: 2.2106e+00, Acc: 0.1910
  Test  - Acc: 0.1938

==========================
Starting epoch 5/30 at 16:00:51
==========================

Initializing training loop...


Batch 0: grad_norm=3.6617 across 22310154 trainable params
Batch 10: grad_norm=4.1767 across 22310154 trainable params
Batch 20: grad_norm=4.9919 across 22310154 trainable params
Batch 30: grad_norm=2.9773 across 22310154 trainable params
Batch 40: grad_norm=4.1975 across 22310154 trainable params

Epoch 5 completed in 7.47s

Learning rate updated to: 0.000467
Epoch 5 Metrics - DIET Loss: 2.3064e+00, Probe Loss: 2.1730e+00, Accuracy: 0.1710

Starting evaluation on test set:
Test Batch 0: Accuracy=0.3000
Test Batch 10: Accuracy=0.3000
Test Batch 20: Accuracy=0.3000
Test Batch 30: Accuracy=0.3000
Test Batch 40: Accuracy=0.2500
Test Batch 50: Accuracy=0.2500
Test Batch 60: Accuracy=0.5000
Test Batch 70: Accuracy=0.2000
Test Batch 80: Accuracy=0.1000
Test Batch 90: Accuracy=0.1500
Test Batch 100: Accuracy=0.2500
Test Batch 110: Accuracy=0.2000
Test Batch 120: Accuracy=0.1000
Test Batch 130: Accuracy=0.2500
Test Batch 140: Accuracy=0.0500
Test Batch 150: Accuracy=0.1500
Test Batch 160: Accuracy=0.1000
Test Batch 170: Accuracy=0.2500
Test Batch 180: Accuracy=0.1000
Test Batch 190: Accuracy=0.3000
Test Batch 200: Accuracy=0.3500
Test Batch 210: Accuracy=0.0500
Test Batch 220: Accuracy=0.1500
Test Batch 230: Accuracy=0.1500
Test Batch 240: Accuracy=0.2500
Test Batch 250: Accuracy=0.3000
Test Batch 260: Accuracy=0.2000
Test Batch 270: Accuracy=0.1500
Test Batch 280: Accuracy=0.2000
Test Batch 290: Accuracy=0.1500
Test Batch 300: Accuracy=0.1500
Test Batch 310: Accuracy=0.3000
Test Batch 320: Accuracy=0.1500
Test Batch 330: Accuracy=0.3000
Test Batch 340: Accuracy=0.2500
Test Batch 350: Accuracy=0.2000
Test Batch 360: Accuracy=0.0500
Test Batch 370: Accuracy=0.1500
Test Batch 380: Accuracy=0.2000
Test Batch 390: Accuracy=0.2500
Test Batch 400: Accuracy=0.3000
Test Batch 410: Accuracy=0.3000
Test Batch 420: Accuracy=0.2500
Test Batch 430: Accuracy=0.2500
Test Batch 440: Accuracy=0.1000
Test Batch 450: Accuracy=0.4000
Test Batch 460: Accuracy=0.3000
Test Batch 470: Accuracy=0.2500
Test Batch 480: Accuracy=0.3500
Test Batch 490: Accuracy=0.1000

Overall Test Accuracy: 0.2071
Epoch 5/30 summary:
  Train - DIET loss: 2.3064e+00, Probe loss: 2.1730e+00, Acc: 0.1710
  Test  - Acc: 0.2071

Running zero-shot evaluation at epoch 5...
Extracting features for zero-shot evaluation...
Extracting features:   0%|          | 0/500 [00:00<?, ?it/s]
Feature hash: 1715972556923378350 (should change between evaluations)
Features extracted: (10000, 384), time: 21.05s
Running k-NN evaluation...
k-NN accuracy: 0.3991, time: 0.32s
Running k-means clustering evaluation...
k-means ARI: 0.0590, NMI: 0.1246, time: 1.73s
Running linear probe evaluation...
Linear probe accuracy: 0.2418, time: 0.05s
Total zero-shot evaluation time: 23.15s

==========================
Starting epoch 6/30 at 16:01:47
==========================

Initializing training loop...


Batch 0: grad_norm=3.5551 across 22310154 trainable params
Traceback (most recent call last):
  File "C:\Users\Bryan\Desktop\ResearchAI\main.py", line 291, in <module>
    main()
  File "C:\Users\Bryan\Desktop\ResearchAI\main.py", line 288, in main
    train(args)
  File "C:\Users\Bryan\Desktop\ResearchAI\main.py", line 209, in train
    metrics_history, initial_results, final_results = trainer.train(
                                                      ^^^^^^^^^^^^^^
  File "C:\Users\Bryan\Desktop\ResearchAI\training\trainer.py", line 162, in train
    z = self.model(x)  # Original features
        ^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\Desktop\ResearchAI\models\dinov2.py", line 54, in forward
    chunk_output = self.model(x_chunk)
                   ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\transformers\models\dinov2\modeling_dinov2.py", line 696, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\transformers\models\dinov2\modeling_dinov2.py", line 514, in forward
    layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\transformers\models\dinov2\modeling_dinov2.py", line 455, in forward
    self_attention_outputs = self.attention(
                             ^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\transformers\models\dinov2\modeling_dinov2.py", line 328, in forward
    self_outputs = self.attention(hidden_states, head_mask, output_attentions)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\transformers\models\dinov2\modeling_dinov2.py", line 257, in forward
    value_layer = self.transpose_for_scores(self.value(hidden_states))
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Bryan\miniconda3\envs\research\Lib\site-packages\torch\nn\modules\linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
