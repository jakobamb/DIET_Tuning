{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-in-one DIET finetuning experiment\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms  # For image transformations in AIM fallback\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import copy  # Add this import here\n",
    "# Add imports for DINOv2\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import torch.nn.functional as F\n",
    "print(\"Setting up experiment...\")\n",
    "import torchvision.datasets as datasets\n",
    "try:\n",
    "    import medmnist\n",
    "    from medmnist import INFO\n",
    "    MEDMNIST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"MedMNIST not found, install with: pip install medmnist\")\n",
    "    MEDMNIST_AVAILABLE = False\n",
    "# Basic settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from io import BytesIO\n",
    "import wandb\n",
    "\n",
    "def init_wandb(args):\n",
    "    \"\"\"Initialize wandb for experiment tracking\n",
    "    \n",
    "    Args:\n",
    "        args: Dictionary containing experiment configuration parameters\n",
    "    \n",
    "    Returns:\n",
    "        run: wandb run object\n",
    "    \"\"\"\n",
    "    # Create experiment name with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_name = f\"DIET_{args['backbone_type']}_{args['model_size']}_{args['dataset_name']}_{timestamp}\"\n",
    "    \n",
    "    # Initialize wandb run\n",
    "    run = wandb.init(\n",
    "        project=\"DIET-Finetuning\",\n",
    "        name=experiment_name,\n",
    "        config=args,\n",
    "        settings=wandb.Settings(start_method=\"thread\"),\n",
    "        tags=[\n",
    "            args['backbone_type'],\n",
    "            args['model_size'],\n",
    "            args['dataset_name'],\n",
    "            \"DIET\" if args['label_smoothing'] > 0 else \"Baseline\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"WandB initialized: {run.name}\")\n",
    "    return run\n",
    "\n",
    "def log_training_metrics(run, metrics, epoch, lr=None):\n",
    "    \"\"\"Log training metrics to wandb\n",
    "    \n",
    "    Args:\n",
    "        run: wandb run object\n",
    "        metrics: Dictionary of training metrics\n",
    "        epoch: Current epoch number\n",
    "        lr: Current learning rate (optional)\n",
    "    \"\"\"\n",
    "    log_dict = {\n",
    "        \"train/diet_loss\": metrics[\"diet_loss\"],\n",
    "        \"train/probe_loss\": metrics[\"probe_loss\"],\n",
    "        \"train/accuracy\": metrics[\"accuracy\"],\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    \n",
    "    # Add learning rate if provided\n",
    "    if lr is not None:\n",
    "        log_dict[\"train/learning_rate\"] = lr\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    run.log(log_dict)\n",
    "\n",
    "def log_evaluation_metrics(run, metrics, epoch):\n",
    "    \"\"\"Log evaluation metrics to wandb\n",
    "    \n",
    "    Args:\n",
    "        run: wandb run object\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "        epoch: Current epoch number\n",
    "    \"\"\"\n",
    "    log_dict = {\n",
    "        \"eval/accuracy\": metrics[\"accuracy\"],\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    run.log(log_dict)\n",
    "\n",
    "def log_zero_shot_metrics(run, metrics, epoch, initial_metrics=None):\n",
    "    \"\"\"Log zero-shot evaluation metrics to wandb\n",
    "    \n",
    "    Args:\n",
    "        run: wandb run object\n",
    "        metrics: Dictionary of zero-shot metrics\n",
    "        epoch: Current epoch number\n",
    "        initial_metrics: Initial zero-shot metrics for comparison (optional)\n",
    "    \"\"\"\n",
    "    log_dict = {\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    \n",
    "    # Log each zero-shot metric\n",
    "    for metric_name, value in metrics.items():\n",
    "        log_dict[f\"zero_shot/{metric_name}\"] = value\n",
    "        \n",
    "        # Log improvements if initial metrics are provided\n",
    "        if initial_metrics is not None:\n",
    "            improvement = value - initial_metrics[metric_name]\n",
    "            relative_improvement = (improvement / initial_metrics[metric_name]) * 100 if initial_metrics[metric_name] > 0 else float('inf')\n",
    "            log_dict[f\"zero_shot/{metric_name}_improvement\"] = improvement\n",
    "            log_dict[f\"zero_shot/{metric_name}_relative_improvement\"] = relative_improvement\n",
    "    \n",
    "    # Calculate average improvement if initial metrics are provided\n",
    "    if initial_metrics is not None:\n",
    "        improvements = [metrics[m] - initial_metrics[m] for m in metrics.keys()]\n",
    "        avg_improvement = np.mean(improvements)\n",
    "        log_dict[\"zero_shot/average_improvement\"] = avg_improvement\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    run.log(log_dict)\n",
    "\n",
    "def save_model_checkpoint(run, model, optimizer, projection_head, W_probe, W_diet, epoch, metrics, save_dir=\"checkpoints\"):\n",
    "    \"\"\"Save model checkpoint and log it to wandb\n",
    "    \n",
    "    Args:\n",
    "        run: wandb run object\n",
    "        model: The backbone model\n",
    "        optimizer: Optimizer\n",
    "        projection_head: DIET projection head\n",
    "        W_probe: Probe linear layer\n",
    "        W_diet: DIET linear layer\n",
    "        epoch: Current epoch number\n",
    "        metrics: Metrics to determine if this is a best checkpoint\n",
    "        save_dir: Directory to save checkpoints locally\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Determine if this is the best checkpoint based on test accuracy\n",
    "    test_acc = metrics.get(\"test_acc\", 0)\n",
    "    is_best = test_acc > getattr(save_model_checkpoint, \"best_acc\", 0)\n",
    "    if is_best:\n",
    "        save_model_checkpoint.best_acc = test_acc\n",
    "    \n",
    "    # Create checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'projection_head_state_dict': projection_head.state_dict(),\n",
    "        'W_probe_state_dict': W_probe.state_dict(),\n",
    "        'W_diet_state_dict': W_diet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'test_acc': test_acc,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    \n",
    "    # Save checkpoint locally\n",
    "    checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    # Save best checkpoint separately\n",
    "    if is_best:\n",
    "        best_path = os.path.join(save_dir, \"best_checkpoint.pt\")\n",
    "        torch.save(checkpoint, best_path)\n",
    "        \n",
    "        # Log best checkpoint to wandb\n",
    "        best_artifact = wandb.Artifact(\n",
    "            name=f\"best_model_{run.id}\", \n",
    "            type=\"model\",\n",
    "            description=f\"Best model checkpoint (epoch {epoch}, acc={test_acc:.4f})\"\n",
    "        )\n",
    "        best_artifact.add_file(best_path)\n",
    "        run.log_artifact(best_artifact)\n",
    "    \n",
    "    # Log regular checkpoint to wandb every 5 epochs or final epoch\n",
    "    if epoch % 5 == 0 or is_best:\n",
    "        artifact = wandb.Artifact(\n",
    "            name=f\"model_e{epoch}_{run.id}\", \n",
    "            type=\"model\",\n",
    "            description=f\"Model checkpoint from epoch {epoch}\"\n",
    "        )\n",
    "        artifact.add_file(checkpoint_path)\n",
    "        run.log_artifact(artifact)\n",
    "\n",
    "# Initialize static variable for best accuracy\n",
    "save_model_checkpoint.best_acc = 0\n",
    "\n",
    "def log_model_architecture(run, model, projection_head, W_probe, W_diet):\n",
    "    \"\"\"Log model architecture details to wandb\n",
    "    \n",
    "    Args:\n",
    "        run: wandb run object\n",
    "        model: The backbone model\n",
    "        projection_head: DIET projection head\n",
    "        W_probe: Probe linear layer\n",
    "        W_diet: DIET linear layer\n",
    "    \"\"\"\n",
    "    # Log architecture as a text table\n",
    "    architecture_text = \"# Model Architecture\\n\\n\"\n",
    "    \n",
    "    # Count parameters\n",
    "    model_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    projection_params = sum(p.numel() for p in projection_head.parameters())\n",
    "    W_probe_params = sum(p.numel() for p in W_probe.parameters())\n",
    "    W_diet_params = sum(p.numel() for p in W_diet.parameters())\n",
    "    total_params = model_params + projection_params + W_probe_params + W_diet_params\n",
    "    total_trainable = trainable_model_params + projection_params + W_probe_params + W_diet_params\n",
    "    \n",
    "    # Add parameter counts\n",
    "    architecture_text += f\"## Parameter Counts\\n\\n\"\n",
    "    architecture_text += f\"| Component | Total Parameters | Trainable Parameters | % of Total |\\n\"\n",
    "    architecture_text += f\"|-----------|-----------------|----------------------|------------|\\n\"\n",
    "    architecture_text += f\"| Backbone Model | {model_params:,} | {trainable_model_params:,} | {100 * model_params / total_params:.2f}% |\\n\"\n",
    "    architecture_text += f\"| Projection Head | {projection_params:,} | {projection_params:,} | {100 * projection_params / total_params:.2f}% |\\n\"\n",
    "    architecture_text += f\"| Classification Head | {W_probe_params:,} | {W_probe_params:,} | {100 * W_probe_params / total_params:.2f}% |\\n\"\n",
    "    architecture_text += f\"| DIET Head | {W_diet_params:,} | {W_diet_params:,} | {100 * W_diet_params / total_params:.2f}% |\\n\"\n",
    "    architecture_text += f\"| **Total** | **{total_params:,}** | **{total_trainable:,}** | **100%** |\\n\\n\"\n",
    "    \n",
    "    # Log the architecture text\n",
    "    run.log({\"model_architecture\": wandb.Html(architecture_text)})\n",
    "\n",
    "def log_zero_shot_comparison_table(run, metrics_history, tracked_epochs, metrics_list):\n",
    "    \"\"\"Log zero-shot metrics comparison table to wandb\n",
    "    \n",
    "    Args:\n",
    "        run: wandb run object\n",
    "        metrics_history: Dictionary of metrics history\n",
    "        tracked_epochs: List of epochs to include in the table\n",
    "        metrics_list: List of metric names to include\n",
    "    \"\"\"\n",
    "    # Create table data\n",
    "    columns = [\"Epoch\"] + metrics_list\n",
    "    data = []\n",
    "    \n",
    "    for epoch in tracked_epochs:\n",
    "        row = [epoch]\n",
    "        for metric in metrics_list:\n",
    "            row.append(metrics_history[\"zero_shot_metrics\"][epoch][metric])\n",
    "        data.append(row)\n",
    "    \n",
    "    # Create wandb Table\n",
    "    table = wandb.Table(columns=columns, data=data)\n",
    "    \n",
    "    # Log table\n",
    "    run.log({\"zero_shot_comparison\": table})\n",
    "\n",
    "def log_figure_to_wandb(run, figure, name):\n",
    "    \"\"\"Convert matplotlib figure to wandb Image and log it\n",
    "    \n",
    "    Args:\n",
    "        run: wandb run object\n",
    "        figure: Matplotlib figure\n",
    "        name: Name for the logged figure\n",
    "    \"\"\"\n",
    "    # Save figure to a BytesIO object\n",
    "    buf = BytesIO()\n",
    "    figure.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Log figure as an image\n",
    "    run.log({name: wandb.Image(buf)})\n",
    "\n",
    "def create_zero_shot_progression_plot(metrics_history, tracked_epochs, metrics_list):\n",
    "    \"\"\"Create zero-shot metrics progression plot\n",
    "    \n",
    "    Args:\n",
    "        metrics_history: Dictionary of metrics history\n",
    "        tracked_epochs: List of epochs to track\n",
    "        metrics_list: List of metric names to include\n",
    "        \n",
    "    Returns:\n",
    "        fig: Matplotlib figure\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    fig = Figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot each metric's progression\n",
    "    for i, metric in enumerate(metrics_list):\n",
    "        ax = fig.add_subplot(2, 2, i+1)\n",
    "        values = [metrics_history[\"zero_shot_metrics\"][e][metric] for e in tracked_epochs]\n",
    "        ax.plot(tracked_epochs, values, marker='o', linewidth=2)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(f'{metric} Score')\n",
    "        ax.set_title(f'Zero-shot {metric} Progression')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Add initial and final values as text annotations\n",
    "        ax.annotate(f'{values[0]:.4f}', (tracked_epochs[0], values[0]), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "        ax.annotate(f'{values[-1]:.4f}', (tracked_epochs[-1], values[-1]),\n",
    "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Zero-shot Metrics Progression During Training', fontsize=16)\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_training_progress_plot(metrics_history):\n",
    "    \"\"\"Create training progress plot\n",
    "    \n",
    "    Args:\n",
    "        metrics_history: Dictionary of metrics history\n",
    "        \n",
    "    Returns:\n",
    "        fig: Matplotlib figure\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    fig = Figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax1.plot(metrics_history[\"train_loss_diet\"], label=\"DIET Loss\")\n",
    "    ax1.plot(metrics_history[\"train_loss_probe\"], label=\"Probe Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Training Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax2.plot(metrics_history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "    ax2.plot(metrics_history[\"test_acc\"], label=\"Test Accuracy\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_title(\"Model Accuracy\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot zero-shot metrics\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    \n",
    "    # Get initial and final zero-shot metrics\n",
    "    tracked_epochs = sorted(metrics_history[\"zero_shot_metrics\"].keys())\n",
    "    if len(tracked_epochs) >= 2:\n",
    "        initial_epoch = tracked_epochs[0]\n",
    "        final_epoch = tracked_epochs[-1]\n",
    "        \n",
    "        metrics = list(metrics_history[\"zero_shot_metrics\"][initial_epoch].keys())\n",
    "        x = range(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Plot bar chart comparing initial and final metrics\n",
    "        ax3.bar(x, [metrics_history[\"zero_shot_metrics\"][initial_epoch][m] for m in metrics], \n",
    "                width, label='Initial')\n",
    "        ax3.bar([i + width for i in x], [metrics_history[\"zero_shot_metrics\"][final_epoch][m] for m in metrics], \n",
    "                width, label='Final')\n",
    "        ax3.set_xlabel(\"Metrics\")\n",
    "        ax3.set_ylabel(\"Score\")\n",
    "        ax3.set_title(\"Zero-Shot Performance\")\n",
    "        ax3.set_xticks([i + width/2 for i in x])\n",
    "        ax3.set_xticklabels(metrics)\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this function to your codebase to log zero-shot metrics to W&B tables\n",
    "def log_metrics_table(run, metrics_history):\n",
    "    \"\"\"\n",
    "    Log zero-shot metrics to a W&B table.\n",
    "    \n",
    "    Args:\n",
    "        run: The wandb run object\n",
    "        metrics_history: Dictionary containing metrics history with zero_shot_metrics\n",
    "    \"\"\"\n",
    "    # Get all tracked epochs and metrics\n",
    "    tracked_epochs = sorted(metrics_history[\"zero_shot_metrics\"].keys())\n",
    "    metrics_list = list(metrics_history[\"zero_shot_metrics\"][tracked_epochs[0]].keys())\n",
    "    \n",
    "    # Create table columns\n",
    "    columns = [\"epoch\"]\n",
    "    for metric in metrics_list:\n",
    "        columns.extend([f\"{metric}\", f\"{metric}_change\", f\"{metric}_relative_change\"])\n",
    "    \n",
    "    # Create table\n",
    "    zero_shot_table = wandb.Table(columns=columns)\n",
    "    \n",
    "    # Initial values for calculating changes\n",
    "    initial_values = metrics_history[\"zero_shot_metrics\"][0]\n",
    "    \n",
    "    # Add data for each epoch\n",
    "    for epoch in tracked_epochs:\n",
    "        current_metrics = metrics_history[\"zero_shot_metrics\"][epoch]\n",
    "        \n",
    "        # Create a row for this epoch\n",
    "        row = [epoch]\n",
    "        \n",
    "        # Add metrics, absolute change and relative change for each metric\n",
    "        for metric in metrics_list:\n",
    "            current_value = current_metrics[metric]\n",
    "            change = current_value - initial_values[metric]\n",
    "            rel_change = (change / initial_values[metric]) * 100 if initial_values[metric] > 0 else float('inf')\n",
    "            \n",
    "            # Add to row: current value, absolute change, relative change\n",
    "            row.extend([current_value, change, rel_change])\n",
    "        \n",
    "        # Add row to table\n",
    "        zero_shot_table.add_data(*row)\n",
    "    \n",
    "    # Log the table\n",
    "    run.log({\"zero_shot_metrics_table\": zero_shot_table})\n",
    "    \n",
    "    # Also create a summary table for final results\n",
    "    final_metrics = metrics_history[\"zero_shot_metrics\"][tracked_epochs[-1]]\n",
    "    \n",
    "    summary_columns = [\"metric\", \"initial\", \"final\", \"change\", \"relative_change\"]\n",
    "    summary_table = wandb.Table(columns=summary_columns)\n",
    "    \n",
    "    for metric in metrics_list:\n",
    "        initial = initial_values[metric]\n",
    "        final = final_metrics[metric]\n",
    "        change = final - initial\n",
    "        rel_change = (change / initial) * 100 if initial > 0 else float('inf')\n",
    "        \n",
    "        summary_table.add_data(metric, initial, final, change, rel_change)\n",
    "    \n",
    "    # Log the summary table\n",
    "    run.log({\"zero_shot_summary_table\": summary_table})\n",
    "\n",
    "\n",
    "# Add this function to improve visualization of metrics with proper names\n",
    "def log_zero_shot_comparison_table(run, metrics_history, tracked_epochs, metrics_list):\n",
    "    \"\"\"\n",
    "    Create and log a more visually appealing zero-shot metrics comparison table.\n",
    "    \n",
    "    Args:\n",
    "        run: The wandb run object\n",
    "        metrics_history: Dictionary containing metrics history\n",
    "        tracked_epochs: List of epochs to include\n",
    "        metrics_list: List of metrics to include\n",
    "    \"\"\"\n",
    "    # Create a nicely formatted table\n",
    "    metric_names = {\n",
    "        \"knn_acc\": \"K-NN Accuracy\",\n",
    "        \"kmeans_ari\": \"K-Means ARI\",\n",
    "        \"kmeans_nmi\": \"K-Means NMI\",\n",
    "        \"linear_acc\": \"Linear Probe Accuracy\"\n",
    "    }\n",
    "    \n",
    "    # Create nice column headers\n",
    "    columns = [\"Epoch\"]\n",
    "    for metric in metrics_list:\n",
    "        # Use friendly names if available\n",
    "        nice_name = metric_names.get(metric, metric)\n",
    "        columns.append(nice_name)\n",
    "    \n",
    "    # Create the table\n",
    "    table = wandb.Table(columns=columns)\n",
    "    \n",
    "    # Add a row for each epoch\n",
    "    for epoch in tracked_epochs:\n",
    "        row = [epoch]\n",
    "        for metric in metrics_list:\n",
    "            value = metrics_history[\"zero_shot_metrics\"][epoch][metric]\n",
    "            row.append(value)\n",
    "        table.add_data(*row)\n",
    "    \n",
    "    # Log the table\n",
    "    run.log({\"zero_shot_progression\": table})\n",
    "\n",
    "\n",
    "# EXAMPLE USAGE:\n",
    "# Add this where you're already doing your final logging\n",
    "# This should be placed around line 850 in paste.txt where you're\n",
    "# creating other visualizations of your data\n",
    "\n",
    "# Right after the \"Create a table for the report\" section,\n",
    "# add this code to log the tables to W&B:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to your code - this creates a more advanced table with visualization helpers\n",
    "\n",
    "def log_enhanced_metrics_table(run, metrics_history, initial_results, final_results):\n",
    "    \"\"\"\n",
    "    Create an enhanced metrics table that includes visualization markers for easier interpretation.\n",
    "    \n",
    "    Args:\n",
    "        run: The wandb run object\n",
    "        metrics_history: Dictionary with metrics history\n",
    "        initial_results: Dictionary with initial metrics\n",
    "        final_results: Dictionary with final metrics\n",
    "    \"\"\"\n",
    "    # Get all tracked epochs and metrics\n",
    "    tracked_epochs = sorted(metrics_history[\"zero_shot_metrics\"].keys())\n",
    "    metrics_list = list(initial_results.keys())\n",
    "    \n",
    "    # Create columns with epoch, metric name, and values\n",
    "    columns = [\"epoch\", \"metric\", \"value\", \"change_from_init\", \"percent_change\", \"trend\"]\n",
    "    table = wandb.Table(columns=columns)\n",
    "    \n",
    "    # Add data for each epoch and metric\n",
    "    for epoch in tracked_epochs:\n",
    "        epoch_metrics = metrics_history[\"zero_shot_metrics\"][epoch]\n",
    "        \n",
    "        for metric in metrics_list:\n",
    "            current = epoch_metrics[metric]\n",
    "            initial = initial_results[metric]\n",
    "            change = current - initial\n",
    "            percent = (change / initial) * 100 if initial > 0 else 0\n",
    "            \n",
    "            # Create a trend indicator (↑ for improvement, ↓ for decline)\n",
    "            # This is simplistic - for some metrics higher might be worse\n",
    "            trend = \"↑\" if change > 0 else \"↓\" if change < 0 else \"→\"\n",
    "            \n",
    "            # Add row to table\n",
    "            table.add_data(epoch, metric, current, change, percent, trend)\n",
    "    \n",
    "    # Log the table\n",
    "    run.log({\"metrics_detailed_progression\": table})\n",
    "    \n",
    "    # Create a summary table with color indicators\n",
    "    summary_columns = [\"metric\", \"initial\", \"final\", \"absolute_change\", \n",
    "                       \"percent_change\", \"trend\", \"significance\"]\n",
    "    summary_table = wandb.Table(columns=summary_columns)\n",
    "    \n",
    "    # Add each metric to the summary\n",
    "    for metric in metrics_list:\n",
    "        initial = initial_results[metric]\n",
    "        final = final_results[metric]\n",
    "        change = final - initial\n",
    "        percent = (change / initial) * 100 if initial > 0 else 0\n",
    "        \n",
    "        # Create a trend indicator\n",
    "        trend = \"↑\" if change > 0 else \"↓\" if change < 0 else \"→\"\n",
    "        \n",
    "        # Significance level (arbitrary thresholds)\n",
    "        if abs(percent) > 20:\n",
    "            significance = \"High\"\n",
    "        elif abs(percent) > 5:\n",
    "            significance = \"Medium\"\n",
    "        else:\n",
    "            significance = \"Low\"\n",
    "        \n",
    "        # Add to summary table\n",
    "        summary_table.add_data(\n",
    "            metric, initial, final, change, percent, trend, significance\n",
    "        )\n",
    "    \n",
    "    # Log the summary table\n",
    "    run.log({\"metrics_final_summary\": summary_table})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sanity_check_results(run, results_dict, model_type):\n",
    "    \"\"\"\n",
    "    Log sanity check results to W&B tables\n",
    "    \n",
    "    Args:\n",
    "        run: W&B run object\n",
    "        results_dict: Dictionary of sanity check results\n",
    "        model_type: Type of model that was evaluated\n",
    "    \"\"\"\n",
    "    if results_dict is None:\n",
    "        print(f\"No results to log for {model_type}\")\n",
    "        return\n",
    "    \n",
    "    # Extract data\n",
    "    k_values = results_dict['k_values']\n",
    "    accuracies = results_dict['accuracies']\n",
    "    best_acc = results_dict['best_acc']\n",
    "    best_k = results_dict['best_k']\n",
    "    \n",
    "    # Check if we have linear probe results (for IJEPA)\n",
    "    has_linear_probe = 'linear_probe_acc' in results_dict\n",
    "    \n",
    "    # Create accuracy table by k value\n",
    "    k_table = wandb.Table(columns=[\"model\", \"k_value\", \"accuracy\"])\n",
    "    \n",
    "    # Add data for each k value\n",
    "    for k, acc in zip(k_values, accuracies):\n",
    "        k_table.add_data(model_type, k, acc * 100)  # Convert to percentage\n",
    "    \n",
    "    # Log the table\n",
    "    run.log({f\"sanity_check_{model_type}_knn\": k_table})\n",
    "    \n",
    "    # Create summary table\n",
    "    if has_linear_probe:\n",
    "        summary_columns = [\"model\", \"method\", \"accuracy\", \"best_k\"]\n",
    "        summary_table = wandb.Table(columns=summary_columns)\n",
    "        \n",
    "        # Add k-NN result\n",
    "        summary_table.add_data(model_type, \"k-NN\", best_acc * 100, best_k)\n",
    "        \n",
    "        # Add linear probe result\n",
    "        summary_table.add_data(model_type, \"Linear Probe\", \n",
    "                             results_dict['linear_probe_acc'] * 100, \"N/A\")\n",
    "    else:\n",
    "        summary_columns = [\"model\", \"best_k_value\", \"best_accuracy\"]\n",
    "        summary_table = wandb.Table(columns=summary_columns)\n",
    "        summary_table.add_data(model_type, best_k, best_acc * 100)\n",
    "    \n",
    "    # Log the summary table\n",
    "    run.log({f\"sanity_check_{model_type}_summary\": summary_table})\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def log_combined_sanity_check_results(run, results_dict):\n",
    "    \"\"\"\n",
    "    Log a combined table of all sanity check results\n",
    "    \n",
    "    Args:\n",
    "        run: W&B run object\n",
    "        results_dict: Dictionary mapping model types to their sanity check results\n",
    "    \"\"\"\n",
    "    # Create table for combined results\n",
    "    combined_table = wandb.Table(\n",
    "        columns=[\"model\", \"method\", \"best_accuracy\", \"best_k\", \"passed_check\"]\n",
    "    )\n",
    "    \n",
    "    # Expected thresholds for each model type\n",
    "    thresholds = {\n",
    "        \"dinov2\": 0.91,\n",
    "        \"mae\": 0.85,\n",
    "        \"mambavision\": 0.85,\n",
    "        \"ijepa\": 0.85,\n",
    "        \"aim\": 0.75\n",
    "    }\n",
    "    \n",
    "    # Add data for each model\n",
    "    for model_type, results in results_dict.items():\n",
    "        if results is None:\n",
    "            continue\n",
    "            \n",
    "        # Get model's threshold\n",
    "        threshold = thresholds.get(model_type, 0.85)\n",
    "        \n",
    "        # Add k-NN result\n",
    "        best_acc = results['best_acc']\n",
    "        best_k = results['best_k']\n",
    "        passed = best_acc >= threshold\n",
    "        \n",
    "        combined_table.add_data(\n",
    "            model_type, \n",
    "            \"k-NN\", \n",
    "            best_acc * 100,  # Convert to percentage\n",
    "            best_k,\n",
    "            \"✓\" if passed else \"✗\"\n",
    "        )\n",
    "        \n",
    "        # Add linear probe result if available\n",
    "        if 'linear_probe_acc' in results:\n",
    "            linear_acc = results['linear_probe_acc']\n",
    "            linear_passed = linear_acc >= threshold\n",
    "            \n",
    "            combined_table.add_data(\n",
    "                model_type,\n",
    "                \"Linear Probe\",\n",
    "                linear_acc * 100,  # Convert to percentage \n",
    "                \"N/A\",\n",
    "                \"✓\" if linear_passed else \"✗\"\n",
    "            )\n",
    "    \n",
    "    # Log the combined table\n",
    "    run.log({\"sanity_check_combined_results\": combined_table})\n",
    "    \n",
    "    return\n",
    "\n",
    "# Example usage:\n",
    "# sanity_results = {\n",
    "#     \"dinov2\": sanity_results_dinov2,\n",
    "#     \"mae\": sanity_results_mae,\n",
    "#     \"mambavision\": sanity_results_mambavision,\n",
    "#     \"ijepa\": sanity_results_ijepa,\n",
    "#     \"aim\": sanity_results_aim\n",
    "# }\n",
    "# \n",
    "# # Log individual results\n",
    "# for model_type, results in sanity_results.items():\n",
    "#     log_sanity_check_results(run, results, model_type)\n",
    "# \n",
    "# # Log combined results\n",
    "# log_combined_sanity_check_results(run, sanity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_sanity_check(\n",
    "    model_type,\n",
    "    model_size=None,\n",
    "    model_variant=None,\n",
    "    expected_threshold=None,  # Now optional, will be set based on model_type\n",
    "    batch_size=None,          # Now optional, will be set based on model_type\n",
    "    k_values=None,\n",
    "    num_workers=0,\n",
    "    log_to_wandb=True        # Added parameter to control W&B logging\n",
    "):\n",
    "    \"\"\"\n",
    "    Unified sanity check with integrated W&B logging: Evaluate model's zero-shot \n",
    "    performance on CIFAR10 using k-NN.\n",
    "    \n",
    "    Args:\n",
    "        model_type (str): Type of model (\"dinov2\", \"mae\", \"mambavision\", \"ijepa\", \"aim\")\n",
    "        model_size (str, optional): Model size for relevant models. Defaults based on model_type.\n",
    "        model_variant (str, optional): Model variant (for mambavision). Defaults based on model_type.\n",
    "        expected_threshold (float, optional): Expected accuracy threshold. Defaults based on model_type.\n",
    "        batch_size (int, optional): Batch size for data loading. Defaults based on model_type.\n",
    "        k_values (list, optional): List of k values to test. Defaults to [1, 5, 20, 50, 100, 200].\n",
    "        num_workers (int, optional): Number of workers for data loader. Defaults to 0.\n",
    "        log_to_wandb (bool, optional): Whether to log results to W&B. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results containing accuracies, k values, best accuracy, and best k\n",
    "    \"\"\"\n",
    "    if k_values is None:\n",
    "        k_values = [1, 5, 20, 50, 100, 200]\n",
    "    \n",
    "    # Set model-specific parameters based on model type\n",
    "    model_defaults = {\n",
    "        \"dinov2\": {\n",
    "            \"model_size\": \"small\",\n",
    "            \"expected_threshold\": 0.91,\n",
    "            \"batch_size\": 256\n",
    "        },\n",
    "        \"mae\": {\n",
    "            \"model_size\": \"base\",\n",
    "            \"expected_threshold\": 0.85,\n",
    "            \"batch_size\": 256\n",
    "        },\n",
    "        \"mambavision\": {\n",
    "            \"model_variant\": \"T\",\n",
    "            \"expected_threshold\": 0.85,\n",
    "            \"batch_size\": 64\n",
    "        },\n",
    "        \"ijepa\": {\n",
    "            \"model_size\": \"b16_1k\",\n",
    "            \"expected_threshold\": 0.85,\n",
    "            \"batch_size\": 64\n",
    "        },\n",
    "        \"aim\": {\n",
    "            \"model_size\": \"600M\",\n",
    "            \"expected_threshold\": 0.75,\n",
    "            \"batch_size\": 256\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Apply default parameters if not provided\n",
    "    if model_type in model_defaults:\n",
    "        defaults = model_defaults[model_type]\n",
    "        if model_size is None and \"model_size\" in defaults:\n",
    "            model_size = defaults[\"model_size\"]\n",
    "        if model_variant is None and \"model_variant\" in defaults:\n",
    "            model_variant = defaults[\"model_variant\"]\n",
    "        if expected_threshold is None and \"expected_threshold\" in defaults:\n",
    "            expected_threshold = defaults[\"expected_threshold\"]\n",
    "        if batch_size is None and \"batch_size\" in defaults:\n",
    "            batch_size = defaults[\"batch_size\"]\n",
    "    \n",
    "    # Set some safe defaults if model type is not recognized\n",
    "    if expected_threshold is None:\n",
    "        expected_threshold = 0.85\n",
    "    if batch_size is None:\n",
    "        batch_size = 128\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"SANITY CHECK: {model_type.upper()} ZERO-SHOT k-NN ON CIFAR10\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize W&B tracking\n",
    "    run = None\n",
    "    if log_to_wandb:\n",
    "        if wandb.run is None:\n",
    "            # No active run, create one for this sanity check\n",
    "            run_name = f\"sanity_check_{model_type}\"\n",
    "            if model_size:\n",
    "                run_name += f\"_{model_size}\"\n",
    "            elif model_variant:\n",
    "                run_name += f\"_{model_variant}\"\n",
    "                \n",
    "            run = wandb.init(project=\"diet_finetuning\", name=run_name, config={\n",
    "                \"model_type\": model_type,\n",
    "                \"model_size\": model_size,\n",
    "                \"model_variant\": model_variant,\n",
    "                \"expected_threshold\": expected_threshold,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"k_values\": k_values\n",
    "            })\n",
    "        else:\n",
    "            # Use existing run\n",
    "            run = wandb.run\n",
    "            # Log config to existing run\n",
    "            run.config.update({\n",
    "                f\"sanity_{model_type}_model_size\": model_size,\n",
    "                f\"sanity_{model_type}_model_variant\": model_variant,\n",
    "                f\"sanity_{model_type}_expected_threshold\": expected_threshold,\n",
    "                f\"sanity_{model_type}_batch_size\": batch_size,\n",
    "                f\"sanity_{model_type}_k_values\": k_values\n",
    "            }, allow_val_change=True)\n",
    "    \n",
    "    # Load CIFAR10 dataset\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                                         (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "    \n",
    "    cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(cifar_train, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(cifar_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    # Create and load the appropriate model\n",
    "    print(f\"Loading fresh {model_type} model...\")\n",
    "    \n",
    "    try:\n",
    "        if model_type == \"dinov2\":\n",
    "            sanity_model, embedding_dim = get_dinov2_model(device, model_size=model_size)\n",
    "        elif model_type == \"mae\":\n",
    "            sanity_model, embedding_dim = get_mae_model(device, model_size=model_size)\n",
    "        elif model_type == \"mambavision\":\n",
    "            sanity_model, embedding_dim = get_mambavision_model(device, model_variant=model_variant)\n",
    "        elif model_type == \"ijepa\":\n",
    "            sanity_model, embedding_dim = get_ijepa_model(device, model_size=model_size)\n",
    "        elif model_type == \"aim\":\n",
    "            sanity_model, embedding_dim = get_aim_model(device, model_size=model_size)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to load model: {e}\"\n",
    "        print(error_msg)\n",
    "        if log_to_wandb and run:\n",
    "            run.log({f\"sanity_check_{model_type}_error\": error_msg})\n",
    "        return None\n",
    "    \n",
    "    sanity_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Special case for I-JEPA: check if we need to run both kNN and linear probe\n",
    "    run_linear_probe = (model_type == \"ijepa\")\n",
    "    linear_accuracy = None\n",
    "    \n",
    "    # Extract features from training set\n",
    "    print(\"Extracting features from CIFAR10 training set...\")\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(train_loader, desc=\"Extracting train features\"):\n",
    "            x = x.to(device)\n",
    "            try:\n",
    "                feat = sanity_model(x)\n",
    "                train_features.append(feat.cpu().numpy())\n",
    "                train_labels.append(y.numpy())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not train_features:\n",
    "        error_msg = \"No features were extracted. Sanity check failed.\"\n",
    "        print(error_msg)\n",
    "        if log_to_wandb and run:\n",
    "            run.log({f\"sanity_check_{model_type}_error\": error_msg})\n",
    "        return None\n",
    "    \n",
    "    train_features = np.vstack(train_features)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "    \n",
    "    # Extract features from test set\n",
    "    print(\"Extracting features from CIFAR10 test set...\")\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(test_loader, desc=\"Extracting test features\"):\n",
    "            x = x.to(device)\n",
    "            try:\n",
    "                feat = sanity_model(x)\n",
    "                test_features.append(feat.cpu().numpy())\n",
    "                test_labels.append(y.numpy())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not test_features:\n",
    "        error_msg = \"No test features were extracted. Sanity check failed.\"\n",
    "        print(error_msg)\n",
    "        if log_to_wandb and run:\n",
    "            run.log({f\"sanity_check_{model_type}_error\": error_msg})\n",
    "        return None\n",
    "    \n",
    "    test_features = np.vstack(test_features)\n",
    "    test_labels = np.concatenate(test_labels)\n",
    "    \n",
    "    print(f\"Features extracted: {train_features.shape} train, {test_features.shape} test\")\n",
    "    \n",
    "    # Normalize features (important for k-NN)\n",
    "    train_features_normalized = train_features / np.linalg.norm(train_features, axis=1, keepdims=True)\n",
    "    test_features_normalized = test_features / np.linalg.norm(test_features, axis=1, keepdims=True)\n",
    "    \n",
    "    # Run k-NN evaluation\n",
    "    if run_linear_probe:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"k-NN EVALUATION\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nEvaluating k-NN performance:\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"{'k value':<10} {'Accuracy':<10}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_k = 0\n",
    "    accuracies = []\n",
    "    \n",
    "    # Create a W&B table for k-NN results\n",
    "    if log_to_wandb and run:\n",
    "        knn_table = wandb.Table(columns=[\"k_value\", \"accuracy\"])\n",
    "    \n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric='cosine')\n",
    "        knn.fit(train_features_normalized, train_labels)\n",
    "        predictions = knn.predict(test_features_normalized)\n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"{k:<10} {accuracy*100:.2f}%\")\n",
    "        \n",
    "        # Log to W&B table\n",
    "        if log_to_wandb and run:\n",
    "            knn_table.add_data(k, accuracy * 100)  # Convert to percentage\n",
    "        \n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_k = k\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Log the k-NN table to W&B\n",
    "    if log_to_wandb and run:\n",
    "        run.log({f\"sanity_check_{model_type}_knn\": knn_table})\n",
    "    \n",
    "    # Linear probe evaluation for I-JEPA\n",
    "    if run_linear_probe:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"LINEAR PROBE EVALUATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Convert features to PyTorch tensors\n",
    "        train_features_tensor = torch.FloatTensor(train_features).to(device)\n",
    "        train_labels_tensor = torch.LongTensor(train_labels).to(device)\n",
    "        test_features_tensor = torch.FloatTensor(test_features).to(device)\n",
    "        test_labels_tensor = torch.LongTensor(test_labels).to(device)\n",
    "        \n",
    "        # Set up linear probe\n",
    "        num_classes = 10  # CIFAR10 has 10 classes\n",
    "        linear_probe = nn.Linear(embedding_dim, num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(linear_probe.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train linear probe\n",
    "        num_epochs = 50\n",
    "        linear_batch_size = 1024\n",
    "        linear_probe.train()\n",
    "        \n",
    "        # Prepare data for batch training\n",
    "        dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=linear_batch_size, shuffle=True)\n",
    "        \n",
    "        # Create list to track loss for W&B\n",
    "        if log_to_wandb and run:\n",
    "            loss_history = []\n",
    "        \n",
    "        print(\"Training linear probe...\")\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch_features, batch_labels in loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = linear_probe(batch_features)\n",
    "                loss = criterion(logits, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            epoch_loss = total_loss / len(loader)\n",
    "            if log_to_wandb and run:\n",
    "                loss_history.append(epoch_loss)\n",
    "                \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Log linear probe training curve\n",
    "        if log_to_wandb and run:\n",
    "            run.log({f\"sanity_check_{model_type}_linear_probe_loss\": wandb.plot.line(\n",
    "                table=wandb.Table(data=[[i, loss] for i, loss in enumerate(loss_history)],\n",
    "                                  columns=[\"epoch\", \"loss\"]),\n",
    "                x=\"epoch\",\n",
    "                y=\"loss\",\n",
    "                title=\"Linear Probe Training Loss\"\n",
    "            )})\n",
    "        \n",
    "        # Evaluate linear probe\n",
    "        linear_probe.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = linear_probe(test_features_tensor)\n",
    "            predictions = logits.argmax(dim=1).cpu().numpy()\n",
    "            linear_accuracy = accuracy_score(test_labels, predictions)\n",
    "        \n",
    "        print(f\"Linear probe accuracy: {linear_accuracy*100:.2f}%\")\n",
    "        \n",
    "        # Log linear probe accuracy\n",
    "        if log_to_wandb and run:\n",
    "            run.log({f\"sanity_check_{model_type}_linear_probe_acc\": linear_accuracy * 100})\n",
    "    \n",
    "    # Determine if sanity check passed\n",
    "    passed_check = best_acc >= expected_threshold\n",
    "    status = \"PASSED ✓\" if passed_check else \"FAILED ✗\"\n",
    "    \n",
    "    print(f\"Best accuracy: {best_acc*100:.2f}% (k={best_k})\")\n",
    "    print(f\"Sanity check status: {status}\")\n",
    "    print(f\"Expected accuracy: >{expected_threshold*100}%, Achieved: {best_acc*100:.2f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Log summary results to W&B\n",
    "    if log_to_wandb and run:\n",
    "        run.log({\n",
    "            f\"sanity_check_{model_type}_best_acc\": best_acc * 100,\n",
    "            f\"sanity_check_{model_type}_best_k\": best_k,\n",
    "            f\"sanity_check_{model_type}_passed\": passed_check\n",
    "        })\n",
    "        \n",
    "        # Create summary table\n",
    "        summary_table = wandb.Table(\n",
    "            columns=[\"model\", \"method\", \"best_accuracy\", \"best_k\", \"threshold\", \"passed_check\"]\n",
    "        )\n",
    "        \n",
    "        # Add k-NN row\n",
    "        summary_table.add_data(\n",
    "            model_type, \n",
    "            \"k-NN\", \n",
    "            best_acc * 100, \n",
    "            best_k,\n",
    "            expected_threshold * 100,\n",
    "            \"✓\" if passed_check else \"✗\"\n",
    "        )\n",
    "        \n",
    "        # Add linear probe row if applicable\n",
    "        if run_linear_probe and linear_accuracy is not None:\n",
    "            linear_passed = linear_accuracy >= expected_threshold\n",
    "            summary_table.add_data(\n",
    "                model_type,\n",
    "                \"Linear Probe\",\n",
    "                linear_accuracy * 100,\n",
    "                \"N/A\",\n",
    "                expected_threshold * 100,\n",
    "                \"✓\" if linear_passed else \"✗\"\n",
    "            )\n",
    "        \n",
    "        # Log the summary table\n",
    "        run.log({f\"sanity_check_{model_type}_summary\": summary_table})\n",
    "    \n",
    "    # Create visualization plot\n",
    "    if run_linear_probe and linear_accuracy is not None:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # Plot k-NN results\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(k_values, [acc*100 for acc in accuracies], marker='o', linewidth=2)\n",
    "        plt.axhline(y=expected_threshold*100, color='r', linestyle='--', label=f'{expected_threshold*100}% threshold')\n",
    "        plt.xlabel('k value')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'{model_type.upper()} Zero-Shot k-NN Performance on CIFAR10')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.xticks(k_values)\n",
    "        \n",
    "        # Plot comparison of methods\n",
    "        plt.subplot(1, 2, 2)\n",
    "        methods = ['k-NN (best)', 'Linear Probe']\n",
    "        method_accuracies = [best_acc*100, linear_accuracy*100]\n",
    "        \n",
    "        plt.bar(methods, method_accuracies, color=['blue', 'orange'])\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Zero-Shot Evaluation Methods Comparison')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.axhline(y=expected_threshold*100, color='r', linestyle='--', label=f'{expected_threshold*100}% threshold')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add text on top of bars\n",
    "        for i, v in enumerate(method_accuracies):\n",
    "            plt.text(i, v+1, f\"{v:.2f}%\", ha='center')\n",
    "    else:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(k_values, [acc*100 for acc in accuracies], marker='o', linewidth=2)\n",
    "        plt.axhline(y=expected_threshold*100, color='r', linestyle='--', label=f'Expected threshold ({expected_threshold*100}%)')\n",
    "        plt.xlabel('k value')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title(f'{model_type.upper()} Zero-Shot k-NN Performance on CIFAR10')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.xticks(k_values)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log the figure to W&B\n",
    "    if log_to_wandb and run:\n",
    "        run.log({f\"sanity_check_{model_type}_plot\": wandb.Image(plt)})\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Prepare return value\n",
    "    results = {\n",
    "        'accuracies': accuracies,\n",
    "        'k_values': k_values,\n",
    "        'best_acc': best_acc,\n",
    "        'best_k': best_k,\n",
    "        'passed_check': passed_check,\n",
    "        'expected_threshold': expected_threshold\n",
    "    }\n",
    "    \n",
    "    if run_linear_probe and linear_accuracy is not None:\n",
    "        results['linear_probe_acc'] = linear_accuracy\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# sanity_results_dinov2 = unified_sanity_check(\"dinov2\")  # W&B logging included\n",
    "# \n",
    "# # If you don't want W&B logging for a specific run:\n",
    "# sanity_results_mae = unified_sanity_check(\"mae\", log_to_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_aim_model(device, model_size=\"600M\"):\n",
    "    \"\"\"Create AIM model using the properly installed AIM package\n",
    "    \n",
    "    Args:\n",
    "        device: The device to load the model on\n",
    "        model_size: Size of the model - \"600M\", \"1B\", \"3B\", or \"7B\"\n",
    "        \n",
    "    Returns:\n",
    "        model: The wrapped AIM model\n",
    "        embedding_dim: The ACTUAL embedding dimension from the loaded model\n",
    "    \"\"\"\n",
    "    print(f\"Loading AIM-{model_size} model...\")\n",
    "    \n",
    "    # These are just reference values, we'll detect the actual dimension\n",
    "    dim_map = {\n",
    "        \"600M\": 768,    # Reference value, will be overridden by actual dimension\n",
    "        \"1B\": 1024,\n",
    "        \"3B\": 1536,\n",
    "        \"7B\": 2048\n",
    "    }\n",
    "    \n",
    "    # Map model size to model ID\n",
    "    model_map = {\n",
    "        \"600M\": \"apple/aim-600M\",\n",
    "        \"1B\": \"apple/aim-1B\",\n",
    "        \"3B\": \"apple/aim-3B\",\n",
    "        \"7B\": \"apple/aim-7B\"\n",
    "    }\n",
    "    \n",
    "    model_id = model_map[model_size]\n",
    "    \n",
    "    try:\n",
    "        # Use the proper AIM imports\n",
    "        from aim.v1.torch.models import AIMForImageClassification\n",
    "        from aim.v1.torch.data import val_transforms\n",
    "        \n",
    "        # Load the model and transforms\n",
    "        print(f\"Loading AIM model: {model_id}\")\n",
    "        base_model = AIMForImageClassification.from_pretrained(model_id)\n",
    "        transform = val_transforms()\n",
    "        print(f\"Successfully loaded AIM model: {model_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading AIM model: {e}\")\n",
    "        raise ValueError(f\"Failed to load AIM model: {e}\")\n",
    "    \n",
    "    # UNFREEZE ALL PARAMETERS\n",
    "    print(\"Unfreezing all AIM parameters...\")\n",
    "    unfrozen_params = 0\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "        unfrozen_params += 1\n",
    "    print(f\"Unfrozen {unfrozen_params} parameters in AIM backbone\")\n",
    "    \n",
    "    # Define wrapper\n",
    "    class AIMWrapper(nn.Module):\n",
    "        def __init__(self, model, transform):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.transform = transform\n",
    "            self._feature_dim_detected = False\n",
    "            self._feature_dim = None\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Make x require gradients to force gradient flow\n",
    "            x = x.detach().requires_grad_(True)\n",
    "            \n",
    "            # Process smaller batches if needed\n",
    "            batch_size = x.shape[0]\n",
    "            if batch_size > 8 and x.device.type == 'cuda':\n",
    "                # Process in chunks to save memory\n",
    "                outputs_list = []\n",
    "                chunk_size = 4 if model_size in [\"3B\", \"7B\"] else 8\n",
    "                \n",
    "                for i in range(0, batch_size, chunk_size):\n",
    "                    # Get batch chunk\n",
    "                    x_chunk = x[i:i+chunk_size]\n",
    "                    \n",
    "                    # Resize to expected input (224x224)\n",
    "                    if x_chunk.shape[-1] != 224:\n",
    "                        x_chunk = F.interpolate(x_chunk, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    \n",
    "                    # Extract features\n",
    "                    features = self._extract_features(x_chunk)\n",
    "                    outputs_list.append(features)\n",
    "                \n",
    "                return torch.cat(outputs_list, dim=0)\n",
    "            else:\n",
    "                # Standard processing for smaller batches\n",
    "                if x.shape[-1] != 224:\n",
    "                    x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Extract features\n",
    "                features = self._extract_features(x)\n",
    "                return features\n",
    "                \n",
    "        def _extract_features(self, x):\n",
    "            \"\"\"Extract features from the AIM model\"\"\"\n",
    "            # Get model output\n",
    "            output = self.model(x)\n",
    "            \n",
    "            # Check output on first run only\n",
    "            if not self._feature_dim_detected:\n",
    "                print(f\"AIM model output type: {type(output)}\")\n",
    "                if isinstance(output, tuple):\n",
    "                    print(f\"Output tuple length: {len(output)}\")\n",
    "                    print(f\"Features shape: {output[1].shape}\")\n",
    "                    self._feature_dim = output[1].shape[1]\n",
    "                else:\n",
    "                    print(f\"Output shape: {output.shape}\")\n",
    "                    self._feature_dim = output.shape[1]\n",
    "                \n",
    "                print(f\"Detected feature dimension: {self._feature_dim}\")\n",
    "                self._feature_dim_detected = True\n",
    "            \n",
    "            # Extract features\n",
    "            if isinstance(output, tuple) and len(output) >= 2:\n",
    "                return output[1]  # Features\n",
    "            else:\n",
    "                # Fallback\n",
    "                return output\n",
    "            \n",
    "        @property\n",
    "        def feature_dim(self):\n",
    "            \"\"\"Get the detected feature dimension\"\"\"\n",
    "            if self._feature_dim is None:\n",
    "                # This will force feature dimension detection with a dummy input\n",
    "                with torch.no_grad():\n",
    "                    dummy_input = torch.randn(1, 3, 224, 224).to(next(self.parameters()).device)\n",
    "                    _ = self._extract_features(dummy_input)\n",
    "            return self._feature_dim\n",
    "    \n",
    "    model = AIMWrapper(base_model, transform).to(device)\n",
    "    \n",
    "    # Detect the actual embedding dimension\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    embedding_dim = model.feature_dim\n",
    "    print(f\"AIM-{model_size} loaded. Detected embedding dimension: {embedding_dim}\")\n",
    "    \n",
    "    return model, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MambaVision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mambavision_model(device, model_variant=\"T\"):\n",
    "    \"\"\"Create MambaVision model using direct feature extraction approach\n",
    "    \n",
    "    Args:\n",
    "        device: The device to put the model on\n",
    "        model_variant: Model variant (T, T2, S, B, L, L2, etc.) or full name\n",
    "        \n",
    "    Returns:\n",
    "        model: Wrapped MambaVision model for feature extraction\n",
    "        embedding_dim: Embedding dimension of the model\n",
    "    \"\"\"\n",
    "    # Map model variants to their configurations based on the documentation\n",
    "    model_configs = {\n",
    "        # ImageNet-1K models\n",
    "        \"T\": {\"id\": \"nvidia/MambaVision-T-1K\", \"dim\": 512, \"res\": 224, \"params\": 31.8},\n",
    "        \"T2\": {\"id\": \"nvidia/MambaVision-T2-1K\", \"dim\": 512, \"res\": 224, \"params\": 35.1},\n",
    "        \"S\": {\"id\": \"nvidia/MambaVision-S-1K\", \"dim\": 768, \"res\": 224, \"params\": 50.1},\n",
    "        \"B\": {\"id\": \"nvidia/MambaVision-B-1K\", \"dim\": 1024, \"res\": 224, \"params\": 97.7},\n",
    "        \"L\": {\"id\": \"nvidia/MambaVision-L-1K\", \"dim\": 1280, \"res\": 224, \"params\": 227.9},\n",
    "        \"L2\": {\"id\": \"nvidia/MambaVision-L2-1K\", \"dim\": 1408, \"res\": 224, \"params\": 241.5},\n",
    "        \n",
    "        # ImageNet-21K models\n",
    "        \"B-21K\": {\"id\": \"nvidia/MambaVision-B-21K\", \"dim\": 1024, \"res\": 224, \"params\": 97.7},\n",
    "        \"L-21K\": {\"id\": \"nvidia/MambaVision-L-21K\", \"dim\": 1280, \"res\": 224, \"params\": 227.9},\n",
    "        \"L2-512-21K\": {\"id\": \"nvidia/MambaVision-L2-512-21K\", \"dim\": 1408, \"res\": 512, \"params\": 241.5},\n",
    "        \"L3-256-21K\": {\"id\": \"nvidia/MambaVision-L3-256-21K\", \"dim\": 1568, \"res\": 256, \"params\": 739.6},\n",
    "        \"L3-512-21K\": {\"id\": \"nvidia/MambaVision-L3-512-21K\", \"dim\": 1568, \"res\": 512, \"params\": 739.6},\n",
    "    }\n",
    "    \n",
    "    # Handle full model names too (e.g., \"MambaVision-T\" or just \"T\")\n",
    "    if model_variant.startswith(\"MambaVision-\"):\n",
    "        model_variant = model_variant[12:]  # Remove \"MambaVision-\" prefix\n",
    "    \n",
    "    if model_variant not in model_configs:\n",
    "        raise ValueError(f\"Model variant {model_variant} not supported. Choose from {list(model_configs.keys())}\")\n",
    "    \n",
    "    config = model_configs[model_variant]\n",
    "    model_id = config[\"id\"]\n",
    "    embedding_dim = config[\"dim\"]\n",
    "    input_res = config[\"res\"]\n",
    "    \n",
    "    print(f\"Loading MambaVision-{model_variant} model...\")\n",
    "    print(f\"Model ID: {model_id}\")\n",
    "    print(f\"Embedding dimension: {embedding_dim}\")\n",
    "    print(f\"Input resolution: {input_res}x{input_res}\")\n",
    "    print(f\"Parameter count: {config['params']} million\")\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoModel\n",
    "        \n",
    "        # Use AutoModel for feature extraction\n",
    "        base_model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "        print(f\"Successfully loaded MambaVision model\")\n",
    "        \n",
    "        # Unfreeze all parameters\n",
    "        print(\"Unfreezing all MambaVision parameters...\")\n",
    "        unfrozen_params = 0\n",
    "        for param in base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "            unfrozen_params += 1\n",
    "        print(f\"Unfrozen {unfrozen_params} parameters in MambaVision backbone\")\n",
    "        \n",
    "        # Create a wrapper class for feature extraction\n",
    "        class MambaVisionWrapper(nn.Module):\n",
    "            def __init__(self, model, input_res, emb_dim):\n",
    "                super().__init__()\n",
    "                self.model = model\n",
    "                self.input_res = input_res\n",
    "                self.emb_dim = emb_dim\n",
    "                \n",
    "            def forward(self, x):\n",
    "                # Make x require gradients to force gradient flow\n",
    "                x = x.detach().requires_grad_(True)\n",
    "                \n",
    "                # Process in small batches to save memory\n",
    "                batch_size = x.shape[0]\n",
    "                if batch_size > 4 and x.device.type == 'cuda':  # Use very small batch size for large models\n",
    "                    outputs_list = []\n",
    "                    for i in range(0, batch_size, 4):\n",
    "                        # Get batch chunk\n",
    "                        x_chunk = x[i:i+4]\n",
    "                        \n",
    "                        # Resize to expected input size\n",
    "                        if x_chunk.shape[-1] != self.input_res:\n",
    "                            x_chunk = F.interpolate(x_chunk, size=(self.input_res, self.input_res), \n",
    "                                                   mode='bilinear', align_corners=False)\n",
    "                        \n",
    "                        # Extract features from model\n",
    "                        try:\n",
    "                            # MambaVision AutoModel returns (avg_pool, features)\n",
    "                            avg_pool, _ = self.model(x_chunk)\n",
    "                            outputs_list.append(avg_pool)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in forward pass: {e}\")\n",
    "                            # Return zeros if there's an error\n",
    "                            dummy = torch.zeros((x_chunk.size(0), self.emb_dim), device=x_chunk.device)\n",
    "                            outputs_list.append(dummy)\n",
    "                    \n",
    "                    return torch.cat(outputs_list, dim=0)\n",
    "                else:\n",
    "                    # Process as a single batch\n",
    "                    if x.shape[-1] != self.input_res:\n",
    "                        x = F.interpolate(x, size=(self.input_res, self.input_res), \n",
    "                                         mode='bilinear', align_corners=False)\n",
    "                    \n",
    "                    try:\n",
    "                        # MambaVision AutoModel returns (avg_pool, features)\n",
    "                        avg_pool, _ = self.model(x)\n",
    "                        return avg_pool\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in forward pass: {e}\")\n",
    "                        return torch.zeros((x.size(0), self.emb_dim), device=x.device)\n",
    "        \n",
    "        # Create and return wrapped model\n",
    "        model = MambaVisionWrapper(base_model, input_res, embedding_dim).to(device)\n",
    "        return model, embedding_dim\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up MambaVision: {e}\")\n",
    "        raise ValueError(f\"Failed to load MambaVision. Consider using DINOv2 or MAE instead. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae_model(device, model_size=\"base\"):\n",
    "    \"\"\"Create MAE model with memory optimization\"\"\"\n",
    "    print(f\"Loading MAE-{model_size} model...\")\n",
    "    \n",
    "    # Model size to embedding dimension mapping\n",
    "    dim_map = {\n",
    "        \"base\": 768,      # ViT-Base dimension\n",
    "        \"large\": 1024,    # ViT-Large dimension\n",
    "        \"huge\": 1280      # ViT-Huge dimension\n",
    "    }\n",
    "    \n",
    "    # Map model size to Hugging Face model ID\n",
    "    model_map = {\n",
    "        \"base\": \"facebook/vit-mae-base\",\n",
    "        \"large\": \"facebook/vit-mae-large\",  # This may need verification\n",
    "        \"huge\": \"facebook/vit-mae-huge\"     # This may need verification\n",
    "    }\n",
    "    \n",
    "    model_id = model_map[model_size]\n",
    "    \n",
    "    try:\n",
    "        # Import the required classes\n",
    "        from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
    "        \n",
    "        # Load processor and model\n",
    "        processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "        base_model = ViTMAEForPreTraining.from_pretrained(model_id)\n",
    "        print(f\"Successfully loaded MAE model from {model_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MAE model: {e}\")\n",
    "        raise ValueError(f\"Could not load MAE model. Please check if transformers is installed.\")\n",
    "    \n",
    "    # UNFREEZE ALL PARAMETERS - exactly like your DINOv2 function\n",
    "    print(\"Unfreezing all MAE parameters...\")\n",
    "    unfrozen_params = 0\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "        unfrozen_params += 1\n",
    "    print(f\"Unfrozen {unfrozen_params} parameters in MAE backbone\")\n",
    "    \n",
    "    # Define wrapper with same structure as DINOv2Wrapper\n",
    "    class MAEWrapper(nn.Module):\n",
    "        def __init__(self, model, processor):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.processor = processor\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Make x require gradients to force gradient flow\n",
    "            x = x.detach().requires_grad_(True)\n",
    "            \n",
    "            # Process smaller batches if needed\n",
    "            batch_size = x.shape[0]\n",
    "            if batch_size > 16 and x.device.type == 'cuda':\n",
    "                # Process in chunks to save memory\n",
    "                outputs_list = []\n",
    "                for i in range(0, batch_size, 16):\n",
    "                    # Get batch chunk\n",
    "                    x_chunk = x[i:i+16]\n",
    "                    # Resize to expected input (224x224)\n",
    "                    if x_chunk.shape[-1] != 224:\n",
    "                        x_chunk = F.interpolate(x_chunk, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    \n",
    "                    # For feature extraction, we use the encoder part of MAE\n",
    "                    features = self.model.vit(x_chunk).last_hidden_state[:, 0]  # CLS token\n",
    "                    outputs_list.append(features)\n",
    "                \n",
    "                return torch.cat(outputs_list, dim=0)\n",
    "            else:\n",
    "                # Standard processing for smaller batches\n",
    "                if x.shape[-1] != 224:\n",
    "                    x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # For feature extraction, we use the encoder part of MAE\n",
    "                features = self.model.vit(x).last_hidden_state[:, 0]  # CLS token\n",
    "                return features\n",
    "    \n",
    "    model = MAEWrapper(base_model, processor).to(device)\n",
    "    embedding_dim = dim_map[model_size]\n",
    "    \n",
    "    print(f\"MAE-{model_size} loaded. Embedding dimension: {embedding_dim}\")\n",
    "    return model, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DinoV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dinov2_model(device, model_size=\"small\"):\n",
    "    \"\"\"Create DINOv2 model with memory optimization\"\"\"\n",
    "    print(f\"Loading DINOv2-{model_size} model...\")\n",
    "    \n",
    "    # Model size to embedding dimension mapping\n",
    "    dim_map = {\n",
    "        \"small\": 384,\n",
    "        \"base\": 768,\n",
    "        \"large\": 1024,\n",
    "        \"giant\": 1536\n",
    "    }\n",
    "    \n",
    "    # Load model and processor\n",
    "    model_name = f\"facebook/dinov2-{model_size}\"\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    base_model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Disable gradient checkpointing as it may interfere with gradient flow\n",
    "    # base_model.gradient_checkpointing_enable()  # Comment out or remove this line\n",
    "    \n",
    "    # UNFREEZE ALL PARAMETERS\n",
    "    print(\"Unfreezing all DINOv2 parameters...\")\n",
    "    unfrozen_params = 0\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "        unfrozen_params += 1\n",
    "    print(f\"Unfrozen {unfrozen_params} parameters in DINOv2 backbone\")\n",
    "    \n",
    "    class DINOv2Wrapper(nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.processor = processor\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Make x require gradients to force gradient flow\n",
    "            x = x.detach().requires_grad_(True)\n",
    "            \n",
    "            # Process smaller batches if needed\n",
    "            batch_size = x.shape[0]\n",
    "            if batch_size > 16 and x.device.type == 'cuda':\n",
    "                # Process in chunks to save memory\n",
    "                outputs_list = []\n",
    "                for i in range(0, batch_size, 16):\n",
    "                    # Get batch chunk\n",
    "                    x_chunk = x[i:i+16]\n",
    "                    # Resize to expected input (224x224)\n",
    "                    if x_chunk.shape[-1] != 224:\n",
    "                        x_chunk = F.interpolate(x_chunk, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    # Process chunk WITHOUT autocast and with gradient tracking\n",
    "                    chunk_output = self.model(x_chunk)\n",
    "                    outputs_list.append(chunk_output.last_hidden_state[:, 0])\n",
    "                return torch.cat(outputs_list, dim=0)\n",
    "            else:\n",
    "                # Standard processing for smaller batches\n",
    "                if x.shape[-1] != 224:\n",
    "                    x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                # Process WITHOUT autocast and with gradient tracking\n",
    "                outputs = self.model(x)\n",
    "                return outputs.last_hidden_state[:, 0]\n",
    "    \n",
    "    model = DINOv2Wrapper(base_model).to(device)\n",
    "    embedding_dim = dim_map[model_size]\n",
    "    \n",
    "    print(f\"DINOv2-{model_size} loaded. Embedding dimension: {embedding_dim}\")\n",
    "    return model, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IJEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ijepa_model(device, model_size):\n",
    "    \"\"\"\n",
    "    Create I-JEPA model using the Hugging Face transformers library\n",
    "    with the correct model identifiers and dimensions.\n",
    "    \n",
    "    Args:\n",
    "        device: The device to put the model on\n",
    "        model_size: Model size, one of \"b16_1k\", \"l14_22k\", \"h14_1k\", etc.\n",
    "        \n",
    "    Returns:\n",
    "        model: I-JEPA model wrapped in a custom wrapper\n",
    "        embedding_dim: Embedding dimension of the model\n",
    "    \"\"\"\n",
    "    # Map model size to exact model IDs based on the available options and their correct dimensions\n",
    "    model_map = {\n",
    "        \"b16_1k\": {\"id\": \"facebook/ijepa_vith16_1k\", \"dim\": 1280, \"img_size\": 448},\n",
    "        \"b16_22k\": {\"id\": \"facebook/ijepa_vitg16_22k\", \"dim\": 1280, \"img_size\": 448},\n",
    "        \"l14_22k\": {\"id\": \"facebook/ijepa_vith14_22k\", \"dim\": 1280, \"img_size\": 448},\n",
    "        \"h14_1k\": {\"id\": \"facebook/ijepa_vith14_1k\", \"dim\": 1280, \"img_size\": 448},\n",
    "    }\n",
    "    \n",
    "    if model_size not in model_map:\n",
    "        raise ValueError(f\"Model size {model_size} not supported. Choose from {list(model_map.keys())}\")\n",
    "    \n",
    "    model_id = model_map[model_size][\"id\"]\n",
    "    embedding_dim = model_map[model_size][\"dim\"]\n",
    "    img_size = model_map[model_size][\"img_size\"]\n",
    "    \n",
    "    print(f\"Loading I-JEPA model {model_id} using transformers...\")\n",
    "    print(f\"Embedding dimension: {embedding_dim}, Image size: {img_size}x{img_size}\")\n",
    "    \n",
    "    try:\n",
    "        # Import the required libraries\n",
    "        from transformers import AutoModel\n",
    "        \n",
    "        # Load the model with the correct ID\n",
    "        base_model = AutoModel.from_pretrained(model_id)\n",
    "        print(f\"Successfully loaded I-JEPA model {model_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading I-JEPA model: {e}\")\n",
    "        raise ValueError(f\"Could not load I-JEPA model. Please check if transformers is installed and the model ID is correct.\")\n",
    "    \n",
    "    # Unfreeze all parameters as requested\n",
    "    print(\"Unfreezing all I-JEPA parameters...\")\n",
    "    unfrozen_params = 0\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "        unfrozen_params += 1\n",
    "    print(f\"Unfrozen {unfrozen_params} parameters in I-JEPA backbone\")\n",
    "    \n",
    "    # Define wrapper class that uses the correct image size\n",
    "    class IJEPAWrapper(nn.Module):\n",
    "        def __init__(self, model, img_size=448):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.img_size = img_size\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Make x require gradients for proper gradient flow\n",
    "            x = x.detach().requires_grad_(True)\n",
    "            \n",
    "            # Process smaller batches if needed (for memory efficiency)\n",
    "            batch_size = x.shape[0]\n",
    "            if batch_size > 8 and x.device.type == 'cuda':  # Reduced batch size for large images\n",
    "                # Process in chunks\n",
    "                outputs_list = []\n",
    "                for i in range(0, batch_size, 8):\n",
    "                    # Get batch chunk\n",
    "                    x_chunk = x[i:i+8]\n",
    "                    # Resize to expected input size (448x448 by default)\n",
    "                    if x_chunk.shape[-1] != self.img_size:\n",
    "                        x_chunk = F.interpolate(x_chunk, size=(self.img_size, self.img_size), \n",
    "                                               mode='bilinear', align_corners=False)\n",
    "                    \n",
    "                    try:\n",
    "                        # Forward pass with the correct image size\n",
    "                        outputs = self.model(x_chunk)\n",
    "                        # Extract embeddings using mean pooling\n",
    "                        features = outputs.last_hidden_state.mean(dim=1)\n",
    "                        outputs_list.append(features)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in forward pass: {e}\")\n",
    "                        try:\n",
    "                            # Try with interpolate_pos_encoding=True\n",
    "                            outputs = self.model(x_chunk, interpolate_pos_encoding=True)\n",
    "                            features = outputs.last_hidden_state.mean(dim=1)\n",
    "                            outputs_list.append(features)\n",
    "                        except Exception as e2:\n",
    "                            print(f\"Second attempt failed: {e2}\")\n",
    "                            # Return zeros as a last resort to avoid crashing\n",
    "                            dummy_features = torch.zeros((x_chunk.size(0), embedding_dim), device=x_chunk.device)\n",
    "                            outputs_list.append(dummy_features)\n",
    "                \n",
    "                return torch.cat(outputs_list, dim=0)\n",
    "            else:\n",
    "                # Standard processing for smaller batches\n",
    "                if x.shape[-1] != self.img_size:\n",
    "                    x = F.interpolate(x, size=(self.img_size, self.img_size), \n",
    "                                     mode='bilinear', align_corners=False)\n",
    "                \n",
    "                try:\n",
    "                    # Forward pass with the correct image size\n",
    "                    outputs = self.model(x)\n",
    "                    # Extract embeddings using mean pooling\n",
    "                    features = outputs.last_hidden_state.mean(dim=1)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in forward pass: {e}\")\n",
    "                    try:\n",
    "                        # Try with interpolate_pos_encoding=True\n",
    "                        outputs = self.model(x, interpolate_pos_encoding=True)\n",
    "                        features = outputs.last_hidden_state.mean(dim=1)\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Second attempt failed: {e2}\")\n",
    "                        # Return zeros as a last resort to avoid crashing\n",
    "                        features = torch.zeros((x.size(0), embedding_dim), device=x.device)\n",
    "                \n",
    "                return features\n",
    "    \n",
    "    # Create and return wrapped model\n",
    "    model = IJEPAWrapper(base_model, img_size=img_size).to(device)\n",
    "    print(f\"I-JEPA model wrapper created with image size {img_size}x{img_size}\")\n",
    "    return model, embedding_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramaters Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set seed for reproducibility\n",
    "#torch.manual_seed(42)\n",
    "#np.random.seed(42)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Hyperparameters you can easily modify\n",
    "num_epoch = 30         # Number of training epochs\n",
    "batch_size = 20         # Batch size: dinov2 128, aim:0.01 \n",
    "da_strength = 1         # 3\n",
    "lr =   5e-4          # resnesT: 1e-4, 5e-4 Dinov2:  1e-6(terrible),1e-7 (didntwork,5e-4 (dino For cifar best so far) \n",
    "weight_decay = 0.05     # 0.05\n",
    "label_smoothing = 0.3    # resnest: 0.3, 0.5, Dinov2:  label_smoothing = 0.1  # Lower from 0.3\n",
    "limit_data = 1000     # or np.inf for full dataset# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetWithIndices(Dataset):\n",
    "    def __init__(self, dataset, num_diet_classes=200):\n",
    "        self.dataset = dataset\n",
    "        self.num_diet_classes = num_diet_classes\n",
    "        # Assign each sample to one of num_diet_classes\n",
    "        self.class_assignments = torch.randint(0, num_diet_classes, (len(dataset),))\n",
    "        \n",
    "    def __getitem__(self, n):\n",
    "        # Convert tensor index to int if needed\n",
    "        if isinstance(n, torch.Tensor):\n",
    "            n = int(n.item())\n",
    "        \n",
    "        # Get sample from wrapped dataset\n",
    "        item = self.dataset[n]\n",
    "        \n",
    "        # Handle different return formats\n",
    "        if isinstance(item, tuple) and len(item) >= 2:\n",
    "            x, y = item[0], item[1]\n",
    "        else:\n",
    "            x = item\n",
    "            y = torch.tensor(0)  # Default label if not provided\n",
    "        \n",
    "        # Ensure label is a proper tensor with correct dimension\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        # Make sure y has the right dimension (not a scalar)\n",
    "        if y.dim() == 0:\n",
    "            y = y.view(1)\n",
    "        \n",
    "        # Ensure diet class is a proper tensor with correct dimension\n",
    "        diet_class = self.class_assignments[n]\n",
    "        if diet_class.dim() == 0:\n",
    "            diet_class = diet_class.view(1)\n",
    "        \n",
    "        return x, y, diet_class\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset))\n",
    "# Add after your existing functions\n",
    "\n",
    "def get_dataset(dataset_name=\"cifar10\", root='./data'):\n",
    "    \"\"\"\n",
    "    Load the specified dataset with predetermined statistics.\n",
    "    \"\"\"\n",
    "    # Predetermined mean and std values for common datasets\n",
    "    dataset_stats = {\n",
    "        \"cifar10\": {\n",
    "            \"mean\": (0.4914, 0.4822, 0.4465),\n",
    "            \"std\": (0.2470, 0.2435, 0.2616),\n",
    "            \"input_size\": 32,\n",
    "            \"is_rgb\": True\n",
    "        },\n",
    "        \"pathmnist\": {\n",
    "            \"mean\": (0.5, 0.5, 0.5),\n",
    "            \"std\": (0.5, 0.5, 0.5),\n",
    "            \"input_size\": 28,\n",
    "            \"is_rgb\": True\n",
    "        },\n",
    "        \"chestmnist\": {\n",
    "            \"mean\": (0.4984),\n",
    "            \"std\": (0.2483),\n",
    "            \"input_size\": 28,\n",
    "            \"is_rgb\": False\n",
    "        },\n",
    "        \"dermamnist\": {\n",
    "            \"mean\": (0.7634, 0.5423, 0.5698),\n",
    "            \"std\": (0.0841, 0.1246, 0.1043),\n",
    "            \"input_size\": 28,\n",
    "            \"is_rgb\": True\n",
    "        },\n",
    "        \"octmnist\": {\n",
    "            \"mean\": (0.1778),\n",
    "            \"std\": (0.1316),\n",
    "            \"input_size\": 28,\n",
    "            \"is_rgb\": False\n",
    "        },\n",
    "        \"pneumoniamnist\": {\n",
    "            \"mean\": (0.5060),\n",
    "            \"std\": (0.2537),\n",
    "            \"input_size\": 28,\n",
    "            \"is_rgb\": False\n",
    "        },\n",
    "        \"plantnet300k\": {\n",
    "            \"mean\": (0.485, 0.456, 0.406),  # ImageNet stats as starting point\n",
    "            \"std\": (0.229, 0.224, 0.225),\n",
    "            \"input_size\": 224,  # PlantNet images are resized to 224\n",
    "            \"is_rgb\": True\n",
    "        },\n",
    "        \"galaxy10_decals\": {\n",
    "            \"mean\": (0.097, 0.097, 0.097),  # Approximate for astronomy images (dark background)\n",
    "            \"std\": (0.174, 0.164, 0.156),   # Astronomical images have different distribution\n",
    "            \"input_size\": 256,  # Original image size is 256x256\n",
    "            \"is_rgb\": True\n",
    "        },\n",
    "\n",
    "        \"crop14_balance\": {\n",
    "            # Based on the dataset card, images are rescaled to a maximum side length of 512.\n",
    "            \"mean\": (0.485, 0.456, 0.406),  # Using ImageNet stats as a placeholder\n",
    "            \"std\": (0.229, 0.224, 0.225),\n",
    "            \"input_size\": 512,\n",
    "            \"is_rgb\": True\n",
    "        }\n",
    "    } \n",
    "    \n",
    "    # Define improved HuggingFace dataset wrapper\n",
    "    from torch.utils.data import Dataset\n",
    "    import torchvision.transforms as transforms\n",
    "    from PIL import Image\n",
    "        \n",
    "    class HFImageDataset(Dataset):\n",
    "        def __init__(self, hf_dataset, transform=None, input_size=224):\n",
    "            self.dataset = hf_dataset\n",
    "            self.transform = transform\n",
    "            self.input_size = input_size\n",
    "            self.resize = transforms.Resize((input_size, input_size))\n",
    "            \n",
    "        def __len__(self):\n",
    "            return int(len(self.dataset))\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            # Convert idx to an integer if needed.\n",
    "            if isinstance(idx, torch.Tensor):\n",
    "                idx = int(idx.item())\n",
    "            item = self.dataset[idx]\n",
    "            image = item['image']\n",
    "            label = item.get('label', item.get('labels'))\n",
    "            if not isinstance(image, Image.Image):\n",
    "                try:\n",
    "                    image = Image.fromarray(image)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Unexpected image format at index {idx}: {e}\")\n",
    "            image = self.resize(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "    \n",
    "    # Load dataset based on name\n",
    "    if dataset_name.lower() == \"cifar10\":\n",
    "        train_dataset = datasets.CIFAR10(root=root, train=True, download=True)\n",
    "        test_dataset = datasets.CIFAR10(root=root, train=False, download=True)\n",
    "        num_classes = 10\n",
    "        \n",
    "    elif MEDMNIST_AVAILABLE and dataset_name.lower() in INFO.keys():\n",
    "        data_flag = dataset_name.lower()\n",
    "        info = INFO[data_flag]\n",
    "        DataClass = getattr(medmnist, info['python_class'])\n",
    "        \n",
    "        # Get dataset information\n",
    "        num_classes = len(info['label'])\n",
    "        \n",
    "        train_dataset = DataClass(split='train', download=True, root=root)\n",
    "        test_dataset = DataClass(split='test', download=True, root=root)\n",
    "        \n",
    "        print(f\"Dataset: {info['description']}\")\n",
    "        print(f\"Task: {info['task']}\")\n",
    "        print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    elif dataset_name.lower() == \"plantnet300k\":\n",
    "        # For PlantNet300K, we'll use HuggingFace datasets\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            \n",
    "            print(\"Loading PlantNet300K dataset from HuggingFace...\")\n",
    "            \n",
    "            # Load the dataset\n",
    "            dataset = load_dataset(\"mikehemberger/plantnet300K\")\n",
    "            \n",
    "            # Number of classes is 85 according to the dataset card\n",
    "            num_classes = 85\n",
    "            \n",
    "            # Get the input_size from stats\n",
    "            input_size = dataset_stats[\"plantnet300k\"][\"input_size\"]\n",
    "            \n",
    "            # Create train and test datasets with consistent sizing\n",
    "            train_dataset = HFImageDataset(dataset['train'], input_size=input_size)\n",
    "            \n",
    "            # Use validation set as test set\n",
    "            if 'validation' in dataset:\n",
    "                test_dataset = HFImageDataset(dataset['validation'], input_size=input_size)\n",
    "            else:\n",
    "                test_dataset = HFImageDataset(dataset['test'], input_size=input_size)\n",
    "            \n",
    "            print(f\"PlantNet300K loaded: {len(train_dataset)} training, {len(test_dataset)} test samples\")\n",
    "            print(f\"Number of classes: {num_classes}\")\n",
    "            print(f\"All images will be resized to {input_size}x{input_size}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            raise ImportError(\"HuggingFace datasets library is required for PlantNet300K. Install with 'pip install datasets'\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading PlantNet300K dataset: {e}\")\n",
    "    \n",
    "    elif dataset_name.lower() == \"galaxy10_decals\":\n",
    "        # For Galaxy10 DECals, we'll use HuggingFace datasets\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            \n",
    "            print(\"Loading Galaxy10 DECals dataset from HuggingFace...\")\n",
    "            \n",
    "            # Load the dataset\n",
    "            dataset = load_dataset(\"matthieulel/galaxy10_decals\")\n",
    "            \n",
    "            # Number of classes is 10 according to the dataset card\n",
    "            num_classes = 10\n",
    "            \n",
    "            # Get the input_size from stats\n",
    "            input_size = dataset_stats[\"galaxy10_decals\"][\"input_size\"]\n",
    "            \n",
    "            # Create train and test datasets with consistent sizing\n",
    "            train_dataset = HFImageDataset(dataset['train'], input_size=input_size)\n",
    "            test_dataset = HFImageDataset(dataset['test'], input_size=input_size)\n",
    "            \n",
    "            print(f\"Galaxy10 DECals loaded: {len(train_dataset)} training, {len(test_dataset)} test samples\")\n",
    "            print(f\"Number of classes: {num_classes}\")\n",
    "            print(f\"All images will be resized to {input_size}x{input_size}\")\n",
    "            print(\"Galaxy class labels:\")\n",
    "            print(\"0: Disturbed Galaxies\")\n",
    "            print(\"1: Merging Galaxies\")\n",
    "            print(\"2: Round Smooth Galaxies\")\n",
    "            print(\"3: In-between Round Smooth Galaxies\") \n",
    "            print(\"4: Cigar Shaped Smooth Galaxies\")\n",
    "            print(\"5: Barred Spiral Galaxies\")\n",
    "            print(\"6: Unbarred Tight Spiral Galaxies\")\n",
    "            print(\"7: Unbarred Loose Spiral Galaxies\")\n",
    "            print(\"8: Edge-on Galaxies without Bulge\")\n",
    "            print(\"9: Edge-on Galaxies with Bulge\")\n",
    "            \n",
    "        except ImportError:\n",
    "            raise ImportError(\"HuggingFace datasets library is required for Galaxy10 DECals. Install with 'pip install datasets'\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading Galaxy10 DECals dataset: {e}\")\n",
    "\n",
    "\n",
    "    elif dataset_name.lower() == \"crop14_balance\":\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            print(\"Loading crop14_balance dataset from Hugging Face (gary109/crop14_balance)...\")\n",
    "            dataset = load_dataset(\"gary109/crop14_balance\")\n",
    "            # Use the provided splits; here, 'train' and 'validation' are available\n",
    "            train_dataset_hf = dataset[\"train\"]\n",
    "            test_dataset_hf = dataset[\"validation\"]\n",
    "            num_classes = 14  # As given in the features (\"14 classes\") \n",
    "            input_size = dataset_stats[\"crop14_balance\"][\"input_size\"]\n",
    "            train_dataset = HFImageDataset(train_dataset_hf, transform=None, input_size=input_size)\n",
    "            test_dataset = HFImageDataset(test_dataset_hf, transform=None, input_size=input_size)\n",
    "            print(f\"crop14_balance loaded: {len(train_dataset)} training, {len(test_dataset)} test samples\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading crop14_balance dataset: {e}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported or MedMNIST not installed\")\n",
    "    \n",
    "    \n",
    "    # Get stats from our predefined dictionary\n",
    "    stats = dataset_stats.get(dataset_name.lower(), {\n",
    "        \"mean\": (0.5,),\n",
    "        \"std\": (0.5,),\n",
    "        \"input_size\": 28,\n",
    "        \"is_rgb\": False\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "    return train_dataset, test_dataset, num_classes, stats[\"input_size\"], stats[\"mean\"], stats[\"std\"], stats[\"is_rgb\"]\n",
    "\n",
    "def calculate_dataset_stats(dataset, batch_size=64, max_samples=10000):\n",
    "    \"\"\"Calculate mean and std for dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: PyTorch dataset or HuggingFace dataset\n",
    "        batch_size: Batch size for loading\n",
    "        max_samples: Maximum number of samples to use (for large datasets)\n",
    "    \n",
    "    Returns:\n",
    "        mean, std as lists\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader, Subset\n",
    "    import random\n",
    "    \n",
    "    # Limit samples for large datasets\n",
    "    if hasattr(dataset, '__len__') and len(dataset) > max_samples:\n",
    "        indices = random.sample(range(len(dataset)), max_samples)\n",
    "        dataset_subset = Subset(dataset, indices)\n",
    "    else:\n",
    "        dataset_subset = dataset\n",
    "    \n",
    "    # Create a copy of the dataset with only ToTensor transform\n",
    "    if hasattr(dataset, 'transform'):\n",
    "        # Standard PyTorch dataset\n",
    "        original_transform = dataset.transform\n",
    "        dataset.transform = torchvision.transforms.ToTensor()\n",
    "    elif hasattr(dataset, 'dataset') and hasattr(dataset.dataset, 'transform'):\n",
    "        # Handle subset case\n",
    "        original_transform = dataset.dataset.transform\n",
    "        dataset.dataset.transform = torchvision.transforms.ToTensor()\n",
    "    else:\n",
    "        # Create a wrapper for HuggingFace datasets or other types\n",
    "        class StatsDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, original_dataset):\n",
    "                self.dataset = original_dataset\n",
    "                self.transform = torchvision.transforms.ToTensor()\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.dataset)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                if hasattr(self.dataset, '__getitem__'):\n",
    "                    item = self.dataset[idx]\n",
    "                    if isinstance(item, tuple) and len(item) >= 2:\n",
    "                        img, label = item[0], item[1]\n",
    "                    else:\n",
    "                        # For HuggingFace datasets\n",
    "                        img = item['image']\n",
    "                        label = item.get('label', 0)\n",
    "                else:\n",
    "                    # Fallback for unusual dataset structures\n",
    "                    raise ValueError(\"Dataset structure not supported for statistics calculation\")\n",
    "                \n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                \n",
    "                return img, label\n",
    "        \n",
    "        dataset_subset = StatsDataset(dataset_subset)\n",
    "        original_transform = None\n",
    "    \n",
    "    # Create loader\n",
    "    loader = DataLoader(\n",
    "        dataset_subset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    \n",
    "    print(\"Calculating dataset statistics...\")\n",
    "    for data, _ in tqdm(loader):\n",
    "        # Check if data is already a tensor\n",
    "        if not isinstance(data, torch.Tensor):\n",
    "            print(f\"Warning: Expected tensor but got {type(data)}. Skipping batch.\")\n",
    "            continue\n",
    "            \n",
    "        # Handle both single-channel and multi-channel images\n",
    "        if data.dim() == 3:  # [batch, height, width]\n",
    "            data = data.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        # Mean over batch, height and width, but not over channels\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Restore original transform\n",
    "    if hasattr(dataset, 'transform') and original_transform is not None:\n",
    "        dataset.transform = original_transform\n",
    "    elif hasattr(dataset, 'dataset') and hasattr(dataset.dataset, 'transform') and original_transform is not None:\n",
    "        dataset.dataset.transform = original_transform\n",
    "    \n",
    "    # Calculate mean and std\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_squared_sum / num_batches - mean**2) ** 0.5\n",
    "    \n",
    "    return mean.tolist(), std.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_eval(net, test_loader, num_classes, eval_id=None):\n",
    "    \"\"\"Evaluate model using zero-shot methods\"\"\"\n",
    "    if eval_id is None:\n",
    "        eval_id = int(time.time()) % 10000\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Extracting features for zero-shot evaluation...\")\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extracting features\"):\n",
    "            # Unpack batch flexibly\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
    "                x, y, _ = batch\n",
    "            elif isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "                x, y = batch\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected batch structure\")\n",
    "            x = x.to(device)\n",
    "            feat = net(x)\n",
    "            features.append(feat.cpu().numpy())\n",
    "            labels.append(y.numpy())\n",
    "\n",
    "    features = np.vstack(features)\n",
    "    labels = np.concatenate(labels).ravel()\n",
    "\n",
    "    feature_hash = hash(str(features[:3].sum()))\n",
    "    print(f\"Feature hash: {feature_hash} (should change between evaluations)\")\n",
    "    print(f\"Features extracted: {features.shape}, time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    results = {}\n",
    "    print(\"Running k-NN evaluation...\")\n",
    "    knn_time = time.time()\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(features, labels)\n",
    "    knn_pred = knn.predict(features)\n",
    "    results[\"knn_acc\"] = accuracy_score(labels, knn_pred)\n",
    "    print(f\"k-NN accuracy: {results['knn_acc']:.4f}, time: {time.time() - knn_time:.2f}s\")\n",
    "\n",
    "    print(\"Running k-means clustering evaluation...\")\n",
    "    kmeans_time = time.time()\n",
    "    kmeans = KMeans(n_clusters=num_classes, random_state=0, n_init=10)\n",
    "    cluster_pred = kmeans.fit_predict(features)\n",
    "    results[\"kmeans_ari\"] = adjusted_rand_score(labels, cluster_pred)\n",
    "    results[\"kmeans_nmi\"] = normalized_mutual_info_score(labels, cluster_pred)\n",
    "    print(f\"k-means ARI: {results['kmeans_ari']:.4f}, NMI: {results['kmeans_nmi']:.4f}, time: {time.time() - kmeans_time:.2f}s\")\n",
    "\n",
    "    print(\"Running linear probe evaluation...\")\n",
    "    linear_time = time.time()\n",
    "    X_train, X_test = features[:len(features)//2], features[len(features)//2:]\n",
    "    y_train, y_test = labels[:len(labels)//2], labels[len(labels)//2:]\n",
    "    linear_clf = nn.Linear(features.shape[1], num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(linear_clf.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    linear_clf.train()\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "    for epoch in range(50):\n",
    "        optimizer.zero_grad()\n",
    "        output = linear_clf(X_train_tensor)\n",
    "        loss = criterion(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    linear_clf.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        output = linear_clf(X_test_tensor)\n",
    "        pred = output.argmax(dim=1).cpu().numpy()\n",
    "        results[\"linear_acc\"] = accuracy_score(y_test, pred)\n",
    "\n",
    "    print(f\"Linear probe accuracy: {results['linear_acc']:.4f}, time: {time.time() - linear_time:.2f}s\")\n",
    "    print(f\"Total zero-shot evaluation time: {time.time() - start_time:.2f}s\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset selection - change this to use different datasets\n",
    "dataset_name = \"crop14_balance\"  # Options: \"cifar10\", \"pathmnist\", \"chestmnist\", \"dermamnist\", \"plantnet300k\", galaxy10_decals\n",
    "num_diet_classes = 10  # Adjust based on dataset (use 200 for PlantNet300K), galaxy10_decals, crop14_balance\n",
    "# Get the appropriate dataset\n",
    "print(f\"Loading {dataset_name} dataset...\")\n",
    "training_data_raw, test_data_raw, num_classes, input_size, mean, std, is_rgb = get_dataset(dataset_name)\n",
    "print(f\"Dataset loaded: input_size={input_size}, mean={mean}, std={std}, is_rgb={is_rgb}\")\n",
    "\n",
    "# Set up data transforms\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Create stronger augmentations for training\n",
    "if da_strength > 0:\n",
    "    aug_list = [\n",
    "        torchvision.transforms.RandomResizedCrop(input_size, antialias=True),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "    ]\n",
    "    \n",
    "    if is_rgb and da_strength > 1:\n",
    "        aug_list.extend([\n",
    "            torchvision.transforms.RandomApply([\n",
    "                torchvision.transforms.ColorJitter(0.4, 0.4, 0.4, 0.2)\n",
    "            ], p=0.3),\n",
    "            torchvision.transforms.RandomGrayscale(p=0.2),\n",
    "        ])\n",
    "    \n",
    "    if da_strength > 2 and is_rgb:\n",
    "        aug_list.append(torchvision.transforms.RandomApply([\n",
    "            torchvision.transforms.GaussianBlur((3, 3), (1.0, 2.0))\n",
    "        ], p=0.2))\n",
    "    \n",
    "    aug_list.extend([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    if da_strength > 2:\n",
    "        aug_list.append(torchvision.transforms.RandomErasing(p=0.25))\n",
    "    \n",
    "    train_transform = torchvision.transforms.Compose(aug_list)\n",
    "else:\n",
    "    train_transform = test_transform\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom collate function for handling tensor dimensions in HuggingFace datasets\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function for properly handling tensors with different dimensions\"\"\"\n",
    "    images, labels, diet_classes = zip(*batch)\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    # Handle labels - ensure they're properly batched\n",
    "    if isinstance(labels[0], torch.Tensor):\n",
    "        # Convert to 1D tensors with consistent dimensions\n",
    "        labels = [label.reshape(-1) for label in labels]\n",
    "        labels = torch.cat(labels)\n",
    "    else:\n",
    "        labels = torch.tensor(labels)\n",
    "    \n",
    "    # Handle diet classes - ensure they're properly batched\n",
    "    if isinstance(diet_classes[0], torch.Tensor):\n",
    "        # Convert to 1D tensors with consistent dimensions\n",
    "        diet_classes = [cls.reshape(-1) for cls in diet_classes]\n",
    "        diet_classes = torch.cat(diet_classes)\n",
    "    else:\n",
    "        diet_classes = torch.tensor(diet_classes)\n",
    "    \n",
    "    return images, labels, diet_classes\n",
    "\n",
    "## Apply transforms to the datasets\n",
    "if dataset_name.lower() in [\"plantnet300k\", \"galaxy10_decals\"]:\n",
    "    # For HuggingFace datasets we need to handle the custom dataset wrapper\n",
    "    training_data = training_data_raw  # No deepcopy needed\n",
    "    test_data = test_data_raw\n",
    "    if hasattr(training_data, 'transform'):\n",
    "        training_data.transform = train_transform\n",
    "        test_data.transform = test_transform\n",
    "    else:\n",
    "        print(f\"Note: {dataset_name} dataset structure is using custom transform handling\")\n",
    "else:\n",
    "    # Standard datasets\n",
    "    training_data = copy.deepcopy(training_data_raw)\n",
    "    try:\n",
    "        training_data.transform = train_transform\n",
    "        test_data = copy.deepcopy(test_data_raw)\n",
    "        test_data.transform = test_transform\n",
    "    except AttributeError:\n",
    "        # Handle if dataset doesn't have a transform attribute (like Subset)\n",
    "        print(\"Note: Using dataset that requires special transform handling\")\n",
    "        # This will be handled by the DataLoader\n",
    "\n",
    "# Limit training data if specified\n",
    "if limit_data < np.inf and limit_data < len(training_data):\n",
    "    print(f\"Limiting training data to {limit_data} samples (out of {len(training_data)})\")\n",
    "    indices = torch.randperm(len(training_data))[:limit_data]\n",
    "    training_data = Subset(training_data, indices)\n",
    "else:\n",
    "    print(f\"Using full training set: {len(training_data)} samples\")\n",
    "\n",
    "# --------- ADD THE GALAXY FIX HERE IF NEEDED ---------\n",
    "# Then in your dataset code:\n",
    "if dataset_name.lower() == \"galaxy10_decals\":\n",
    "    print(\"\\n===== REBUILDING GALAXY DATASET FROM SCRATCH =====\")\n",
    "    \n",
    "    # Get the raw dataset again\n",
    "    from datasets import load_dataset\n",
    "    from PIL import Image  # Add the import here as well for safety\n",
    "    \n",
    "    raw_dataset = load_dataset(\"matthieulel/galaxy10_decals\")\n",
    "    train_data = raw_dataset[\"train\"]\n",
    "    test_data = raw_dataset[\"test\"]\n",
    "    \n",
    "    # Build a completely new dataset class from scratch that avoids all dimension issues\n",
    "    class RobustGalaxyDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, hf_dataset, transform=None, diet_classes=100, limit_samples=None):\n",
    "            \"\"\"A robust dataset class that guarantees consistent tensor dimensions\"\"\"\n",
    "            self.dataset = hf_dataset\n",
    "            self.transform = transform\n",
    "            \n",
    "            # Limit samples if requested\n",
    "            if limit_samples is not None and limit_samples < len(hf_dataset):\n",
    "                indices = torch.randperm(len(hf_dataset))[:limit_samples].tolist()\n",
    "                self.indices = indices\n",
    "            else:\n",
    "                self.indices = list(range(len(hf_dataset)))\n",
    "            \n",
    "            # Create diet class assignments - one per sample\n",
    "            self.diet_classes = torch.randint(0, diet_classes, (len(self.indices),))\n",
    "            \n",
    "            # Create resize transform to ensure consistent image sizes\n",
    "            self.resize = torchvision.transforms.Resize((256, 256))\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Get the sample using our saved indices\n",
    "            original_idx = self.indices[idx]\n",
    "            sample = self.dataset[original_idx]\n",
    "            \n",
    "            # Get image and label\n",
    "            image = sample['image']\n",
    "            label = torch.tensor([sample['label']], dtype=torch.long)  # Create as 1D tensor\n",
    "            \n",
    "            # Convert to PIL if needed\n",
    "            if not isinstance(image, Image.Image):\n",
    "                try:\n",
    "                    image = Image.fromarray(image)\n",
    "                except:\n",
    "                    print(f\"Warning: Could not convert image to PIL at index {idx}\")\n",
    "            \n",
    "            # Resize to ensure consistent dimensions\n",
    "            image = self.resize(image)\n",
    "            \n",
    "            # Apply additional transforms\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            # Get diet class for this sample - ensure it's a 1D tensor\n",
    "            diet_class = torch.tensor([self.diet_classes[idx].item()], dtype=torch.long)\n",
    "            \n",
    "            return image, label, diet_class\n",
    "    \n",
    "    # Create new robust training dataset\n",
    "    print(\"Creating robust galaxy dataset...\")\n",
    "    robust_train_dataset = RobustGalaxyDataset(\n",
    "        train_data, \n",
    "        transform=train_transform,\n",
    "        diet_classes=num_diet_classes,\n",
    "        limit_samples=limit_data if limit_data < np.inf else None\n",
    "    )\n",
    "    \n",
    "    # Replace the original wrapped training data\n",
    "    training_data = robust_train_dataset\n",
    "    test_data = RobustGalaxyDataset(\n",
    "        test_data,\n",
    "        transform=test_transform,\n",
    "        diet_classes=num_diet_classes\n",
    "    )\n",
    "    \n",
    "    print(f\"Created robust datasets: {len(training_data)} training, {len(test_data)} test\")\n",
    "    print(\"===== GALAXY DATASET REBUILDING COMPLETE =====\\n\")\n",
    "else:\n",
    "    # For non-Galaxy datasets, use the regular DatasetWithIndices wrapper\n",
    "    training_data = DatasetWithIndices(training_data, num_diet_classes=num_diet_classes)\n",
    "\n",
    "# Print test set size\n",
    "print(f\"Test set size: {len(test_data)} samples\")\n",
    "\n",
    "# --------- CONTINUE WITH DATALOADERS ---------\n",
    "\n",
    "# Create data loaders\n",
    "training_loader = DataLoader(\n",
    "    training_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    drop_last=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, \n",
    "    drop_last=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbones-ModelSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create model - Select backbone\n",
    "backbone_type = \"dinov2\"  # Options: \"resnet50\", \"dinov2\", \"ijepa\",\"MAE\", \"aim\"\n",
    "model_size = \"small\"      # Dinov2-Options: \"small\", \"base\", \"large\", \"giant\", Ijepa-options:\"b16_1k\", \"l14\", \"h14\", MAE-options: \"base\", \"large\", or \"huge\"\n",
    "                        # mambavision: model_size:  \"T\", \"S\", \"B\", \"L\", \"L2\", \"L3\"\n",
    "                        # aim: \"600M\", \"1B\", \"3B\", \"7B\"\n",
    "\n",
    "sanity_results_dinov2 = unified_sanity_check(backbone_type, model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if backbone_type == \"resnet50\":\n",
    "    # Original ResNet50 code\n",
    "    print(\"Creating ResNet50 model with ImageNet pre-training...\")\n",
    "    net = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "    \n",
    "    # Modify for 32x32 images (CIFAR10)\n",
    "    print(\"Adapting ResNet50 for 32x32 images...\")\n",
    "    net.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    net.maxpool = nn.Identity()\n",
    "    embedding_dim = net.fc.in_features\n",
    "    net.fc = nn.Identity()\n",
    "\n",
    "elif backbone_type == \"mae\":\n",
    "    # Use MAE\n",
    "    print(f\"Creating MAE-{model_size} model...\")\n",
    "    net, embedding_dim = get_mae_model(device, model_size=model_size)\n",
    "    # All parameters should be trainable already from the get_mae_model function\n",
    "    # Just add diagnostic to verify this\n",
    "    \n",
    "    # ADD THE DIAGNOSTIC CODE HERE\n",
    "    print(\"\\nChecking trainable parameters...\")\n",
    "    trainable_params_count = 0\n",
    "    total_params_count = 0\n",
    "    trainable_wrapper_params = 0\n",
    "    for name, param in net.named_parameters():\n",
    "        total_params_count += 1\n",
    "        if param.requires_grad:\n",
    "            trainable_params_count += 1\n",
    "            if 'model' in name:  # Parameters within the wrapped MAE model\n",
    "                trainable_wrapper_params += 1\n",
    "    print(f\"Total parameters: {total_params_count}\")\n",
    "    print(f\"Trainable parameters: {trainable_params_count}\")\n",
    "    print(f\"Trainable wrapper MAE parameters: {trainable_wrapper_params}\")\n",
    "\n",
    "elif backbone_type == \"ijepa\":\n",
    "    print(f\"Creating I-JEPA-{model_size} model...\")\n",
    "    net, embedding_dim = get_ijepa_model(device, model_size=model_size)\n",
    "    \n",
    "    # OPTIONAL: Apply strategic freezing to focus training on upper layers\n",
    "    print(\"Applying strategic freezing to I-JEPA...\")\n",
    "    frozen_params = 0\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, param in net.model.named_parameters():\n",
    "        total_params += 1\n",
    "        # Only freeze embeddings, unfreeze all transformer layers\n",
    "        if 'embeddings' in name:  # Only freeze embeddings, not encoder layers\n",
    "            param.requires_grad = False\n",
    "            frozen_params += 1\n",
    "        else:\n",
    "            param.requires_grad = True  # Explicitly set other layers to trainable\n",
    "    \n",
    "    print(f\"Frozen {frozen_params} out of {total_params} parameters\")\n",
    "    \n",
    "    # Add diagnostic to check which layers are trainable\n",
    "    print(\"\\nTrainable layers in I-Jepa:\")\n",
    "    for name, param in net.model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  ✓ {name}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {name}\")\n",
    "\n",
    "     # ADD THE DIAGNOSTIC CODE HERE\n",
    "    print(\"\\nChecking trainable parameters...\")\n",
    "    trainable_params_count = 0\n",
    "    total_params_count = 0\n",
    "    trainable_wrapper_params = 0\n",
    "    for name, param in net.named_parameters():\n",
    "        total_params_count += 1\n",
    "        if param.requires_grad:\n",
    "            trainable_params_count += 1\n",
    "            if 'model' in name:  # Parameters within the wrapped I-jepa model\n",
    "                trainable_wrapper_params += 1\n",
    "    print(f\"Total parameters: {total_params_count}\")\n",
    "    print(f\"Trainable parameters: {trainable_params_count}\")\n",
    "    print(f\"Trainable wrapper I-jepa parameters: {trainable_wrapper_params}\")\n",
    "    \n",
    "    # Rest of your DIET training code remains the same\n",
    "elif backbone_type == \"mambavision\":\n",
    "    # Use MambaVision\n",
    "    print(f\"Creating MambaVision {model_size} model...\")\n",
    "    net, embedding_dim = get_mambavision_model(device, model_variant=model_size)\n",
    "    \n",
    "    # Add diagnostic to check trainable parameters\n",
    "    print(\"\\nChecking trainable parameters...\")\n",
    "    trainable_params_count = 0\n",
    "    total_params_count = 0\n",
    "    trainable_wrapper_params = 0\n",
    "    for name, param in net.named_parameters():\n",
    "        total_params_count += 1\n",
    "        if param.requires_grad:\n",
    "            trainable_params_count += 1\n",
    "            if 'model' in name:  # Parameters within the wrapped model\n",
    "                trainable_wrapper_params += 1\n",
    "    print(f\"Total parameters: {total_params_count}\")\n",
    "    print(f\"Trainable parameters: {trainable_params_count}\")\n",
    "    print(f\"Trainable wrapper MambaVision parameters: {trainable_wrapper_params}\")\n",
    "\n",
    "    \n",
    "elif backbone_type == \"dinov2\":\n",
    "    # Use DINOv2\n",
    "    print(f\"Creating DINOv2-{model_size} model...\")\n",
    "    net, embedding_dim = get_dinov2_model(device, model_size=model_size)\n",
    "\n",
    "    # UNFREEZE MORE LAYERS - Modified freezing strategy\n",
    "    print(\"Applying minimal freezing to DINOv2 (allowing more gradients)...\")\n",
    "    frozen_params = 0\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, param in net.model.named_parameters():\n",
    "        total_params += 1\n",
    "        # Only freeze embeddings, unfreeze all transformer layers\n",
    "        if 'embeddings' in name:  # Only freeze embeddings, not encoder layers\n",
    "            param.requires_grad = False\n",
    "            frozen_params += 1\n",
    "        else:\n",
    "            param.requires_grad = True  # Explicitly set other layers to trainable\n",
    "    \n",
    "    print(f\"Frozen {frozen_params} out of {total_params} parameters\")\n",
    "    \n",
    "    # Add diagnostic to check which layers are trainable\n",
    "    print(\"\\nTrainable layers in DINOv2:\")\n",
    "    for name, param in net.model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  ✓ {name}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {name}\")\n",
    "\n",
    "     # ADD THE DIAGNOSTIC CODE HERE\n",
    "    print(\"\\nChecking trainable parameters...\")\n",
    "    trainable_params_count = 0\n",
    "    total_params_count = 0\n",
    "    trainable_wrapper_params = 0\n",
    "    for name, param in net.named_parameters():\n",
    "        total_params_count += 1\n",
    "        if param.requires_grad:\n",
    "            trainable_params_count += 1\n",
    "            if 'model' in name:  # Parameters within the wrapped DINOv2 model\n",
    "                trainable_wrapper_params += 1\n",
    "    print(f\"Total parameters: {total_params_count}\")\n",
    "    print(f\"Trainable parameters: {trainable_params_count}\")\n",
    "    print(f\"Trainable wrapper DINOv2 parameters: {trainable_wrapper_params}\")\n",
    "\n",
    "elif backbone_type == \"aim\":\n",
    "    # Use AIM (Autoregressive Image Models)\n",
    "    print(f\"Creating AIM-{model_size} model...\")\n",
    "    net, embedding_dim = get_aim_model(device, model_size=model_size)\n",
    "    \n",
    "    # Add diagnostic to check trainable parameters\n",
    "    print(\"\\nChecking trainable parameters...\")\n",
    "    trainable_params_count = 0\n",
    "    total_params_count = 0\n",
    "    trainable_wrapper_params = 0\n",
    "    for name, param in net.named_parameters():\n",
    "        total_params_count += 1\n",
    "        if param.requires_grad:\n",
    "            trainable_params_count += 1\n",
    "            if 'model' in name:  # Parameters within the wrapped AIM model\n",
    "                trainable_wrapper_params += 1\n",
    "    print(f\"Total parameters: {total_params_count}\")\n",
    "    print(f\"Trainable parameters: {trainable_params_count}\")\n",
    "    print(f\"Trainable wrapper AIM parameters: {trainable_wrapper_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The layers are organized in a hierarchical fashion, with earlier layers (0, 1, 2) processing more basic features and later layers (9, 10, 11) handling more complex semantic features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Move model to device\n",
    "net = net.to(device)\n",
    "\n",
    "print(f\"Using {backbone_type} backbone with embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Add projection head definition here\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.norm = nn.BatchNorm1d(hidden_dim)  # Add normalization\n",
    "        self.dropout = nn.Dropout(0.1)  # Add dropout for better generalization\n",
    "        self.layer2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.norm(x)  # Apply normalization\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Create the projection head\n",
    "projection_dim = 256  # You can experiment with this value\n",
    "projection_head = ProjectionHead(embedding_dim, embedding_dim, projection_dim).to(device)\n",
    "\n",
    "\n",
    "# Create heads for DIET and probing\n",
    "W_probe = nn.Linear(embedding_dim, num_classes).to(device)\n",
    "\n",
    "W_diet = nn.Linear(projection_dim, num_diet_classes, bias=False).to(device)\n",
    "\n",
    "# Create optimizer\n",
    "print(f\"Creating optimizer with lr={lr}, weight_decay={weight_decay}\")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(net.parameters()) + list(W_probe.parameters()) + \n",
    "    list(W_diet.parameters()) + list(projection_head.parameters()),\n",
    "    lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "\n",
    "# Add learning rate scheduler\n",
    "print(\"Creating learning rate scheduler (cosine annealing)\")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epoch, eta_min=1e-5) #, eta_min=1e-5\n",
    "\n",
    "\n",
    "# Loss functions\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "criterion_diet = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "# DIET active status\n",
    "is_diet_active = label_smoothing > 0\n",
    "print(f\"DIET is {'active' if is_diet_active else 'inactive'} (label_smoothing={label_smoothing})\")\n",
    "\n",
    "# Run initial zero-shot evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INITIAL ZERO-SHOT EVALUATION (BEFORE TRAINING)\")\n",
    "print(\"=\"*50)\n",
    "initial_time = time.time()\n",
    "###cHANGE THIS \n",
    "initial_results = zero_shot_eval(net, test_loader, num_classes, eval_id=0)\n",
    "#####\n",
    "\n",
    "\n",
    "print(f\"Initial evaluation completed in {time.time() - initial_time:.2f}s\")\n",
    "\n",
    "# Print hyperparameters used in the experiment\n",
    "print(\"==========================================\")\n",
    "print(\"Experiment Hyperparameters:\")\n",
    "print(\"==========================================\")\n",
    "print(f\"Number of Epochs     : {num_epoch}\")\n",
    "print(f\"Batch Size           : {batch_size}\")\n",
    "print(f\"Data Augmentation Strength : {da_strength}\")\n",
    "print(f\"Learning Rate        : {lr}\")\n",
    "print(f\"Weight Decay         : {weight_decay}\")\n",
    "print(f\"Label Smoothing      : {label_smoothing}\")\n",
    "print(f\"Data Limit           : {limit_data}\")\n",
    "print(f\"num_diet_classes           : {num_diet_classes}\")\n",
    "\n",
    "print(\"==========================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set(net, test_loader, device, W_probe):\n",
    "    print(\"\\nStarting evaluation on test set:\")\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        run_acc_test = []\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            # Flexible unpacking: works with (x, y) or (x, y, n)\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                if len(batch) == 3:\n",
    "                    x, y, _ = batch\n",
    "                    print(f\"Test Batch {i}: Unpacked as (x, y, n)\")\n",
    "                elif len(batch) == 2:\n",
    "                    x, y = batch\n",
    "                    print(f\"Test Batch {i}: Unpacked as (x, y)\")\n",
    "                else:\n",
    "                    print(f\"Warning: Unexpected test batch length ({len(batch)}) at batch {i}. Skipping.\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected test batch type at batch {i}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            z = net(x)\n",
    "            logits_probe = W_probe(z)\n",
    "            \n",
    "            # Adjust dimensions if necessary\n",
    "            if y.dim() != logits_probe.argmax(1).dim():\n",
    "                y = y.squeeze() if y.dim() > logits_probe.argmax(1).dim() else y.unsqueeze(0)\n",
    "            batch_acc = torch.mean((y == logits_probe.argmax(1)).float()).item()\n",
    "            run_acc_test.append(batch_acc)\n",
    "            \n",
    "            print(f\"Test Batch {i}: Accuracy={batch_acc:.4f}\")\n",
    "            \n",
    "        test_acc = np.mean(run_acc_test) if run_acc_test else 0\n",
    "        print(f\"\\nOverall Test Accuracy: {test_acc:.4f}\")\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace your training loop with this safe version\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop with extensive error handling\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"STARTING TRAINING FOR {num_epoch} EPOCHS (SAFE MODE)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Device being used: {device}\")\n",
    "print(f\"Dataset: {dataset_name} | Size: {len(training_data)} samples\")\n",
    "print(f\"Number of batches per epoch: {len(training_loader)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Add wandb configuration here\n",
    "# Create experiment configuration dictionary\n",
    "experiment_config = {\n",
    "    # Model parameters\n",
    "    \"backbone_type\": backbone_type,\n",
    "    \"model_size\": model_size,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"projection_dim\": projection_dim,\n",
    "    \n",
    "    # Dataset parameters\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"num_diet_classes\": num_diet_classes,\n",
    "    \"input_size\": input_size,\n",
    "    \"is_rgb\": is_rgb,\n",
    "    \"limit_data\": limit_data if 'limit_data' in locals() else None,\n",
    "    \n",
    "    # Training parameters\n",
    "    \"num_epoch\": num_epoch,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"da_strength\": da_strength if 'da_strength' in locals() else None,\n",
    "    \"label_smoothing\": label_smoothing,\n",
    "    \n",
    "    # DIET-specific settings\n",
    "    \"is_diet_active\": is_diet_active\n",
    "}\n",
    "\n",
    "# Initialize wandb with your configuration\n",
    "run = init_wandb(experiment_config)\n",
    "# Then log model architecture\n",
    "log_model_architecture(run, net, projection_head, W_probe, W_diet)\n",
    "\n",
    "# Then log initial zero-shot results\n",
    "log_zero_shot_metrics(run, initial_results, 0)\n",
    "\n",
    "train_start_time = time.time()\n",
    "epoch_times = []\n",
    "metrics_history = {\n",
    "    \"train_loss_diet\": [],\n",
    "    \"train_loss_probe\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"test_acc\": [],\n",
    "    \"zero_shot_metrics\": {}\n",
    "}\n",
    "metrics_history[\"zero_shot_metrics\"][0] = initial_results  # Store epoch 0 results\n",
    "\n",
    "# ----- Begin Training & Evaluation Loop -----\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start = time.time()\n",
    "    net.train()\n",
    "    run_loss_diet, run_loss_probe, run_acc = [], [], []\n",
    "    \n",
    "    print(f\"\\n==========================\")\n",
    "    print(f\"Starting epoch {epoch+1}/{num_epoch} at {time.strftime('%H:%M:%S')}\")\n",
    "    print(f\"==========================\\n\")\n",
    "    \n",
    "    print(\"Initializing training loop...\")\n",
    "    # Use tqdm progress bar\n",
    "    pbar = tqdm(training_loader, desc=f\"Epoch {epoch+1}/{num_epoch}\", \n",
    "                leave=False)  # Set leave=False to prevent multiple progress bars\n",
    "    \n",
    "    for i, batch in enumerate(pbar):\n",
    "        batch_start = time.time()\n",
    "        # Flexible batch unpacking: support (x, y, n) and (x, y)\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            if len(batch) == 3:\n",
    "                x, y, n = batch\n",
    "                # Don't print this for each batch - too verbose\n",
    "                # print(f\"Batch {i}: Unpacked as (x, y, n)\")\n",
    "            elif len(batch) == 2:\n",
    "                x, y = batch\n",
    "                n = None  # No diet class provided\n",
    "                # Don't print this for each batch - too verbose\n",
    "                # print(f\"Batch {i}: Unpacked as (x, y) – no diet class\")\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected batch length ({len(batch)}) at batch {i}. Skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected batch type at batch {i}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Send tensors to device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # Ensure y is 1D (flatten if needed)\n",
    "        if y.dim() > 1:\n",
    "            y = y.view(-1)\n",
    "        if n is not None:\n",
    "            n = n.to(device).long()\n",
    "            if n.dim() > 1:\n",
    "                n = n.view(-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        z = net(x)  # Original features\n",
    "        z_norm = F.normalize(z, p=2, dim=1)  # L2 normalize\n",
    "        z_proj = projection_head(z_norm)       # Projection through MLP\n",
    "        \n",
    "        temperature = 3\n",
    "        if n is not None:\n",
    "            logits_diet = W_diet(z_proj) / temperature\n",
    "            loss_diet = criterion_diet(logits_diet, n)\n",
    "        else:\n",
    "            loss_diet = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        logits_probe = W_probe(z)\n",
    "        loss_probe = criterion(logits_probe, y)\n",
    "        \n",
    "        # Combine losses dynamically\n",
    "        if n is not None:\n",
    "            if epoch < 15:\n",
    "                loss = 0.6 * loss_diet + 0.4 * loss_probe\n",
    "            elif epoch < 25:\n",
    "                loss = 0.4 * loss_diet + 0.6 * loss_probe\n",
    "            else:\n",
    "                loss = 0.2 * loss_diet + 0.8 * loss_probe\n",
    "        else:\n",
    "            loss = loss_probe\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track gradient norm to confirm parameters are updating\n",
    "        if i % 10 == 0:  # Only check every 10 batches to save computation\n",
    "            total_grad_norm = 0\n",
    "            trainable_params = 0\n",
    "            for name, param in net.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    total_grad_norm += param.grad.norm().item()\n",
    "                    trainable_params += 1\n",
    "            # Print without tqdm interference by using tqdm.write\n",
    "            tqdm.write(f\"Batch {i}: grad_norm={total_grad_norm:.4f} across {trainable_params} trainable params\")\n",
    "        \n",
    "        # Record training metrics\n",
    "        run_loss_diet.append(loss_diet.item())\n",
    "        run_loss_probe.append(loss_probe.item())\n",
    "        preds = logits_probe.argmax(dim=1)\n",
    "        batch_acc = torch.mean((y == preds).float()).item()\n",
    "        run_acc.append(batch_acc)\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        pbar.set_postfix({\n",
    "            'DIET loss': f\"{np.mean(run_loss_diet):.4e}\",\n",
    "            'Probe loss': f\"{np.mean(run_loss_probe):.4e}\",\n",
    "            'Acc': f\"{np.mean(run_acc):.4f}\",\n",
    "            'Batch time': f\"{batch_time:.3f}s\"\n",
    "        })\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_times.append(epoch_time)\n",
    "    print(f\"\\nEpoch {epoch+1} completed in {epoch_time:.2f}s\\n\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_history[\"train_loss_diet\"].append(np.mean(run_loss_diet))\n",
    "    metrics_history[\"train_loss_probe\"].append(np.mean(run_loss_probe))\n",
    "    metrics_history[\"train_acc\"].append(np.mean(run_acc))\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Learning rate updated to: {current_lr:.6f}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Metrics - DIET Loss: {np.mean(run_loss_diet):.4e}, Probe Loss: {np.mean(run_loss_probe):.4e}, Accuracy: {np.mean(run_acc):.4f}\")\n",
    "\n",
    "    # ADD THIS LINE HERE - log training metrics to wandb\n",
    "    log_training_metrics(\n",
    "        run, \n",
    "        {\"diet_loss\": np.mean(run_loss_diet), \"probe_loss\": np.mean(run_loss_probe), \"accuracy\": np.mean(run_acc)}, \n",
    "        epoch+1, \n",
    "        current_lr\n",
    "    )\n",
    "\n",
    "    # ----- Test Evaluation Loop -----\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        run_acc_test = []\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            # Flexible unpacking for test batches (supports (x, y, n) and (x, y))\n",
    "            if isinstance(batch, (tuple, list)):\n",
    "                if len(batch) == 3:\n",
    "                    x, y, _ = batch\n",
    "                    # Removed verbose logging\n",
    "                    # print(f\"Test Batch {i}: Unpacked as (x, y, n)\")\n",
    "                elif len(batch) == 2:\n",
    "                    x, y = batch\n",
    "                    # Removed verbose logging\n",
    "                    # print(f\"Test Batch {i}: Unpacked as (x, y)\")\n",
    "                else:\n",
    "                    raise ValueError(\"Unexpected batch structure in test set\")\n",
    "            else:\n",
    "                raise ValueError(\"Test batch is not a tuple or list\")\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            if y.dim() > 1:\n",
    "                y = y.view(-1)\n",
    "            z = net(x)\n",
    "            logits_probe = W_probe(z)\n",
    "            if y.dim() != logits_probe.argmax(1).dim():\n",
    "                if y.dim() > logits_probe.argmax(1).dim():\n",
    "                    y = y.squeeze()\n",
    "                else:\n",
    "                    y = y.unsqueeze(0)\n",
    "            test_batch_acc = torch.mean((y == logits_probe.argmax(1)).float()).item()\n",
    "            run_acc_test.append(test_batch_acc)\n",
    "            \n",
    "            # Removed verbose batch logging\n",
    "            # print(f\"Test Batch {i}: Accuracy={test_batch_acc:.4f}\")\n",
    "        \n",
    "        test_acc = np.mean(run_acc_test) if run_acc_test else 0\n",
    "        metrics_history[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        #wandb\n",
    "        log_evaluation_metrics(run, {\"accuracy\": test_acc}, epoch+1)\n",
    "    # ----- End Test Evaluation Loop -----\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{num_epoch} summary:\")\n",
    "    print(f\"  Train - DIET loss: {np.mean(run_loss_diet) if run_loss_diet else 0:.4e}, \"\n",
    "          f\"Probe loss: {np.mean(run_loss_probe) if run_loss_probe else 0:.4e}, \"\n",
    "          f\"Acc: {np.mean(run_acc) if run_acc else 0:.4f}\")\n",
    "    print(f\"  Test  - Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    # ----- Zero-Shot Evaluation (every 5 epochs or last epoch) -----\n",
    "    if (epoch + 1) % 5 == 0 or epoch == num_epoch - 1:\n",
    "        print(f\"\\nRunning zero-shot evaluation at epoch {epoch+1}...\")\n",
    "        try:\n",
    "            # Perturb model slightly to force a different evaluation state\n",
    "            for param in net.parameters():\n",
    "                if param.requires_grad:\n",
    "                    with torch.no_grad():\n",
    "                        original_data = param.data.clone()\n",
    "                        param.data.add_(torch.randn_like(param) * 1e-6)  # Tiny perturbation\n",
    "                        break\n",
    "            \n",
    "            # Run your zero-shot evaluation function (assumes it's defined)\n",
    "            epoch_zero_shot = zero_shot_eval(net, test_loader, num_classes, eval_id=epoch+1)\n",
    "\n",
    "            # Restore original parameter\n",
    "            with torch.no_grad():\n",
    "                param.data.copy_(original_data)\n",
    "            \n",
    "            metrics_history[\"zero_shot_metrics\"][epoch+1] = copy.deepcopy(epoch_zero_shot)\n",
    "            \n",
    "            # Log zero-shot metrics to wandb\n",
    "            log_zero_shot_metrics(run, epoch_zero_shot, epoch+1, initial_results)\n",
    "\n",
    "            # Print formatted zero-shot performance table\n",
    "            print(f\"\\nZero-shot Performance at epoch {epoch+1}:\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"{'Metric':<15} {'Initial':<10} {'Current':<10} {'Change':<10} {'Relative %':<10}\")\n",
    "            print(\"-\" * 60)\n",
    "            for metric in epoch_zero_shot.keys():\n",
    "                initial = initial_results[metric]\n",
    "                current = epoch_zero_shot[metric]\n",
    "                change = current - initial\n",
    "                rel_change = (change / initial) * 100 if initial > 0 else float('inf')\n",
    "                print(f\"{metric:<15} {initial:.4f}     {current:.4f}     {change:+.4f}     {rel_change:+.2f}%\")\n",
    "        except Exception as zs_err:\n",
    "            print(f\"Error in zero-shot evaluation: {zs_err}\")\n",
    "    # ----- End Zero-Shot Evaluation -----\n",
    "  \n",
    "    # Add wandb checkpoint saving here\n",
    "    # Save model checkpoint to wandb\n",
    "    checkpoint_metrics = {\n",
    "        \"train_acc\": np.mean(run_acc) if run_acc else 0,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"train_loss_diet\": np.mean(run_loss_diet) if run_loss_diet else 0,\n",
    "        \"train_loss_probe\": np.mean(run_loss_probe) if run_loss_diet else 0,\n",
    "    }\n",
    "    save_model_checkpoint(\n",
    "        run, net, optimizer, projection_head, W_probe, W_diet, \n",
    "        epoch+1, checkpoint_metrics, save_dir=\"checkpoints\"\n",
    "    )\n",
    "\n",
    "# End of Epoch Loop\n",
    "print(f\"\\nTraining completed in {time.time() - train_start_time:.2f}s\")\n",
    "\n",
    "# Remaining code for plotting and evaluation remains unchanged...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(metrics_history[\"train_loss_diet\"], label=\"DIET Loss\")\n",
    "plt.plot(metrics_history[\"train_loss_probe\"], label=\"Probe Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(metrics_history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "plt.plot(metrics_history[\"test_acc\"], label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Final zero-shot evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL ZERO-SHOT EVALUATION (AFTER TRAINING)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "##here\n",
    "\n",
    "# Create visualization of zero-shot metrics progression\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get all metrics and epochs\n",
    "tracked_epochs = sorted(metrics_history[\"zero_shot_metrics\"].keys())\n",
    "metrics_list = list(initial_results.keys())\n",
    "\n",
    "# Plot each metric's progression\n",
    "for i, metric in enumerate(metrics_list):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    values = [metrics_history[\"zero_shot_metrics\"][e][metric] for e in tracked_epochs]\n",
    "    plt.plot(tracked_epochs, values, marker='o', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(f'{metric} Score')\n",
    "    plt.title(f'Zero-shot {metric} Progression')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add initial and final values as text annotations\n",
    "    plt.annotate(f'{values[0]:.4f}', (tracked_epochs[0], values[0]), \n",
    "                 textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.annotate(f'{values[-1]:.4f}', (tracked_epochs[-1], values[-1]),\n",
    "                 textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Zero-shot Metrics Progression During Training', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.savefig(f'{dataset_name}_zeroshot_progression.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add wandb figure and table logging here\n",
    "# Log zero-shot progression plot to wandb\n",
    "zero_shot_fig = plt.gcf()  # Get current figure\n",
    "log_figure_to_wandb(run, zero_shot_fig, \"zero_shot_progression\")\n",
    "\n",
    "# Log zero-shot comparison table\n",
    "log_zero_shot_comparison_table(run, metrics_history, tracked_epochs, metrics_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a table for the report\n",
    "print(\"\\nZero-shot Progression Table:\")\n",
    "print(\"-\"*80)\n",
    "header = \"Epoch\".ljust(10)\n",
    "for metric in metrics_list:\n",
    "    header += f\"{metric}\".ljust(15)\n",
    "print(header)\n",
    "print(\"-\"*80)\n",
    "\n",
    "for epoch in tracked_epochs:\n",
    "    row = f\"{epoch}\".ljust(10)\n",
    "    for metric in metrics_list:\n",
    "        value = metrics_history[\"zero_shot_metrics\"][epoch][metric]\n",
    "        row += f\"{value:.4f}\".ljust(15)\n",
    "    print(row)\n",
    "print(\"-\"*80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ADD THE TABLE LOGGING RIGHT HERE:\n",
    "# Log metrics tables to W&B\n",
    "log_metrics_table(run, metrics_history)\n",
    "log_zero_shot_comparison_table(run, metrics_history, tracked_epochs, metrics_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_time = time.time()\n",
    "final_results = zero_shot_eval(net, test_loader, num_classes, eval_id=num_epoch+1)\n",
    "print(f\"Final evaluation completed in {time.time() - final_time:.2f}s\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add wandb final zero-shot metrics logging here\n",
    "# Log final zero-shot results to wandb\n",
    "log_zero_shot_metrics(run, final_results, num_epoch+1, initial_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate improvements\n",
    "improvements = {\n",
    "    f\"improvement_{k}\": final_results[k] - initial_results[k]\n",
    "    for k in initial_results.keys()\n",
    "}\n",
    "\n",
    "# Plot zero-shot metrics\n",
    "plt.subplot(1, 3, 3)\n",
    "metrics = list(initial_results.keys())\n",
    "x = range(len(metrics))\n",
    "width = 0.35\n",
    "plt.bar(x, [initial_results[m] for m in metrics], width, label='Initial')\n",
    "plt.bar([i + width for i in x], [final_results[m] for m in metrics], width, label='Final')\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Zero-Shot Performance\")\n",
    "plt.xticks([i + width/2 for i in x], metrics)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add wandb figure logging here\n",
    "# Create and log training progress plot to wandb\n",
    "training_fig = plt.gcf()  # Get current figure\n",
    "log_figure_to_wandb(run, training_fig, \"training_progress\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ADD TABLE LOGGING FOR FINAL RESULTS HERE:\n",
    "# Create and log final results table to wandb\n",
    "final_results_table = wandb.Table(\n",
    "    columns=[\"Metric\", \"Initial\", \"Final\", \"Improvement\", \"Relative %\"]\n",
    ")\n",
    "\n",
    "for metric in metrics:\n",
    "    initial = initial_results[metric]\n",
    "    final = final_results[metric]\n",
    "    imp = improvements[f\"improvement_{metric}\"]\n",
    "    rel_imp = (imp / initial) * 100 if initial > 0 else float('inf')\n",
    "    final_results_table.add_data(metric, initial, final, imp, rel_imp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Final sum\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset: {dataset_name.upper()}\")    \n",
    "print(f\"Model: {backbone_type.upper()} {'('+model_size+')' if backbone_type=='dinov2' else '(ImageNet pre-trained)'}\")\n",
    "print(f\"DIET label smoothing: {label_smoothing}\" + (\" (DIET active)\" if is_diet_active else \" (DIET inactive)\"))\n",
    "print(f\"Training samples: {len(training_data)}, Epochs: {num_epoch}\")\n",
    "print(\"\\nZero-shot performance:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(f\"{'Metric':<15} {'Initial':<10} {'Final':<10} {'Improvement':<10} {'Relative %':<10}\")\n",
    "print(\"-\"*60)\n",
    "for metric in metrics:\n",
    "    initial = initial_results[metric]\n",
    "    final = final_results[metric]\n",
    "    imp = improvements[f\"improvement_{metric}\"]\n",
    "    rel_imp = (imp / initial) * 100 if initial > 0 else float('inf')\n",
    "    print(f\"{metric:<15} {initial:.4f}     {final:.4f}     {imp:+.4f}     {rel_imp:+.2f}%\")\n",
    "\n",
    "print(\"\\nCONCLUSION:\")\n",
    "avg_improvement = np.mean([improvements[f\"improvement_{k}\"] for k in initial_results.keys()])\n",
    "if avg_improvement > 0:\n",
    "    print(f\"DIET finetuning {'improved' if is_diet_active else 'would likely improve'} zero-shot performance \" +\n",
    "          f\"by an average of {avg_improvement:.4f} ({(avg_improvement / np.mean(list(initial_results.values()))) * 100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"DIET finetuning {'did not improve' if is_diet_active else 'would likely not improve'} zero-shot performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add wandb summary and finish here\n",
    "# Log summary to wandb\n",
    "summary_text = f\"\"\"\n",
    "# DIET Experiment Summary\n",
    "\n",
    "## Configuration\n",
    "- **Dataset**: {dataset_name.upper()}\n",
    "- **Model**: {backbone_type.upper()} {f\"({model_size})\" if backbone_type in ['dinov2', 'mae', 'ijepa', 'aim'] else '(ImageNet pre-trained)'}\n",
    "- **DIET**: {\"Active\" if is_diet_active else \"Inactive\"} (label_smoothing={label_smoothing})\n",
    "- **Training**: {len(training_data)} samples, {num_epoch} epochs\n",
    "\n",
    "## Zero-shot Performance\n",
    "| Metric | Initial | Final | Improvement | Relative % |\n",
    "|--------|---------|-------|------------|------------|\n",
    "\"\"\"\n",
    "\n",
    "for metric in metrics:\n",
    "    initial = initial_results[metric]\n",
    "    final = final_results[metric]\n",
    "    imp = improvements[f\"improvement_{metric}\"]\n",
    "    rel_imp = (imp / initial) * 100 if initial > 0 else float('inf')\n",
    "    summary_text += f\"| {metric} | {initial:.4f} | {final:.4f} | {imp:+.4f} | {rel_imp:+.2f}% |\\n\"\n",
    "\n",
    "# Log summary to wandb\n",
    "run.log({\"experiment_summary\": wandb.Html(summary_text)})\n",
    "\n",
    "# Update run summary with final metrics\n",
    "run.summary.update({\n",
    "    \"avg_improvement\": avg_improvement,\n",
    "    \"avg_relative_improvement\": (avg_improvement / np.mean(list(initial_results.values()))) * 100 if np.mean(list(initial_results.values())) > 0 else 0,\n",
    "    \"final_test_acc\": test_acc,\n",
    "    \"best_test_acc\": getattr(save_model_checkpoint, \"best_acc\", test_acc),\n",
    "    \"training_time\": time.time() - train_start_time\n",
    "})\n",
    "\n",
    "# Finish the wandb run\n",
    "run.finish()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "def save_experiment_results(model_name, dataset_name, metrics_history, hyperparams, save_dir=\"results\"):\n",
    "    \"\"\"Save experiment results with guaranteed unique folders for each experiment\"\"\"\n",
    "    \n",
    "    # Create directory structure if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate a unique experiment ID (combining timestamp and a UUID)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")  # Added milliseconds\n",
    "    exp_id = f\"{timestamp}_{str(uuid.uuid4())[:8]}\"\n",
    "    \n",
    "    # Create unique experiment folder with key parameters in name\n",
    "    lr_str = f\"lr{hyperparams['lr']:.1e}\"\n",
    "    diet_str = f\"diet{hyperparams['num_diet_classes']}\"\n",
    "    ls_str = f\"ls{hyperparams['label_smoothing']}\"\n",
    "    \n",
    "    # Create folder path with key hyperparameters\n",
    "    exp_folder = f\"{model_name}_{dataset_name}_{lr_str}_{diet_str}_{ls_str}_{exp_id}\"\n",
    "    full_path = f\"{save_dir}/{exp_folder}\"\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "    \n",
    "    # 1. Save experiment description with ALL hyperparameters\n",
    "    with open(f\"{full_path}/experiment_info.json\", 'w') as f:\n",
    "        # Add any additional metadata you want to track\n",
    "        metadata = {\n",
    "            \"model\": model_name,\n",
    "            \"dataset\": dataset_name,\n",
    "            \"experiment_id\": exp_id,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"hyperparameters\": hyperparams\n",
    "        }\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # 2. Save metrics history as JSON\n",
    "    with open(f\"{full_path}/metrics_history.json\", 'w') as f:\n",
    "        # Convert NumPy values to Python native types for JSON serialization\n",
    "        serializable_metrics = {}\n",
    "        for key, value in metrics_history.items():\n",
    "            if key == \"zero_shot_metrics\":\n",
    "                serializable_metrics[key] = {\n",
    "                    str(epoch): {m: float(v) for m, v in metrics.items()}\n",
    "                    for epoch, metrics in value.items()\n",
    "                }\n",
    "            else:\n",
    "                serializable_metrics[key] = [float(v) for v in value]\n",
    "        \n",
    "        json.dump(serializable_metrics, f, indent=2)\n",
    "    \n",
    "    # 3. Create and save progression table as CSV\n",
    "    if \"zero_shot_metrics\" in metrics_history:\n",
    "        epochs = sorted(metrics_history[\"zero_shot_metrics\"].keys())\n",
    "        metrics_list = list(metrics_history[\"zero_shot_metrics\"][epochs[0]].keys())\n",
    "        \n",
    "        table_data = []\n",
    "        for epoch in epochs:\n",
    "            row = {\"epoch\": epoch}\n",
    "            for metric in metrics_list:\n",
    "                row[metric] = metrics_history[\"zero_shot_metrics\"][epoch][metric]\n",
    "            table_data.append(row)\n",
    "        \n",
    "        # Save as CSV\n",
    "        pd.DataFrame(table_data).to_csv(f\"{full_path}/progression_table.csv\", index=False)\n",
    "    \n",
    "    # 4. Save plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Training metrics\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(metrics_history[\"train_loss_diet\"], label=\"DIET Loss\")\n",
    "    plt.plot(metrics_history[\"train_loss_probe\"], label=\"Probe Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(metrics_history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "    plt.plot(metrics_history[\"test_acc\"], label=\"Test Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Zero-shot progression\n",
    "    if \"zero_shot_metrics\" in metrics_history:\n",
    "        tracked_epochs = sorted(metrics_history[\"zero_shot_metrics\"].keys())\n",
    "        metrics_list = list(metrics_history[\"zero_shot_metrics\"][tracked_epochs[0]].keys())\n",
    "        \n",
    "        for i, metric in enumerate(metrics_list[:2]):  # First 2 metrics\n",
    "            plt.subplot(2, 2, 3+i)\n",
    "            values = [metrics_history[\"zero_shot_metrics\"][e][metric] for e in tracked_epochs]\n",
    "            plt.plot(tracked_epochs, values, marker='o', linewidth=2)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(f'{metric}')\n",
    "            plt.title(f'Zero-shot {metric} Progression')\n",
    "            plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{model_name} on {dataset_name} - {diet_str}, {ls_str}\", fontsize=14)\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.savefig(f\"{full_path}/summary_plot.png\", dpi=300)\n",
    "    \n",
    "    # 5. Also save a text summary with key results\n",
    "    with open(f\"{full_path}/results_summary.txt\", 'w') as f:\n",
    "        f.write(f\"Experiment Summary: {model_name} on {dataset_name}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Key Hyperparameters:\\n\")\n",
    "        f.write(f\"- Learning Rate: {hyperparams['lr']}\\n\")\n",
    "        f.write(f\"- DIET Classes: {hyperparams['num_diet_classes']}\\n\")\n",
    "        f.write(f\"- Label Smoothing: {hyperparams['label_smoothing']}\\n\")\n",
    "        f.write(f\"- Weight Decay: {hyperparams['weight_decay']}\\n\\n\")\n",
    "        \n",
    "        if \"zero_shot_metrics\" in metrics_history:\n",
    "            epochs = sorted(metrics_history[\"zero_shot_metrics\"].keys())\n",
    "            initial_metrics = metrics_history[\"zero_shot_metrics\"][epochs[0]]\n",
    "            final_metrics = metrics_history[\"zero_shot_metrics\"][epochs[-1]]\n",
    "            \n",
    "            f.write(\"Zero-Shot Performance:\\n\")\n",
    "            f.write(\"-\"*50 + \"\\n\")\n",
    "            f.write(f\"{'Metric':<15} {'Initial':<10} {'Final':<10} {'Change':<10} {'Relative %':<10}\\n\")\n",
    "            f.write(\"-\"*50 + \"\\n\")\n",
    "            \n",
    "            for metric in initial_metrics.keys():\n",
    "                initial = initial_metrics[metric]\n",
    "                final = final_metrics[metric]\n",
    "                change = final - initial\n",
    "                rel_change = (change / initial) * 100 if initial > 0 else float('inf')\n",
    "                f.write(f\"{metric:<15} {initial:.4f}     {final:.4f}     {change:+.4f}     {rel_change:+.2f}%\\n\")\n",
    "    \n",
    "    print(f\"Experiment results saved to {full_path}\")\n",
    "    return full_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end of your script, after training\n",
    "hyperparams = {\n",
    "    \"num_epoch\": num_epoch,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"da_strength\": da_strength,\n",
    "    \"lr\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"label_smoothing\": label_smoothing,\n",
    "    \"num_diet_classes\": num_diet_classes,\n",
    "    \"temperature\": 0.1,  # Your temperature parameter\n",
    "    \"model_size\": model_size if backbone_type == \"dinov2\" else \"n/a\",\n",
    "    \"freezing_layers\": \"last2\" if backbone_type == \"dinov2\" else \"n/a\"\n",
    "}\n",
    "\n",
    "model_name = f\"{backbone_type}_{model_size}\" if backbone_type == \"dinov2\" else backbone_type\n",
    "save_path = save_experiment_results(model_name, dataset_name, metrics_history, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
